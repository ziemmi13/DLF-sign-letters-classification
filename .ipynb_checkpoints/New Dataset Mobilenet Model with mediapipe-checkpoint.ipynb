{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import lightning as pl\n",
    "import torchmetrics\n",
    "import comet_ml\n",
    "import os\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from comet_ml import Experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from mediapipe_handcrop import MediapipeHandCrop\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE TRAIN AND TEST CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_brightness(image_path):\n",
    "    # Wczytanie obrazu\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Sprawdzenie, czy obraz został poprawnie wczytany\n",
    "    if image is None:\n",
    "        print(\"Nie udało się wczytać obrazu.\")\n",
    "        return None\n",
    "\n",
    "    # Konwersja obrazu do przestrzeni barw HSV\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Ekstrakcja kanału jasności (V)\n",
    "    brightness_channel = hsv_image[:, :, 2]\n",
    "\n",
    "    # Obliczenie średniej jasności\n",
    "    average_brightness = np.mean(brightness_channel)\n",
    "\n",
    "    # print(f\"{average_brightness = }\")\n",
    "\n",
    "    return average_brightness > 135.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pth = r\"C:\\Users\\Hyperbook\\Desktop\\STUDIA\\DLF3\\DLF-sign_letters_classification\\new_dataset\\train\"\n",
    "test_pth = r\"C:\\Users\\Hyperbook\\Desktop\\STUDIA\\DLF3\\DLF-sign_letters_classification\\new_dataset\\test\"\n",
    "\n",
    "mediapipe_validator = MediapipeHandCrop(include_characteristic_vectors=True)\n",
    "\n",
    "def validate_dataset_with_mediapipe(dataset_path, csv_file_path, new_dir):\n",
    "    data = []\n",
    "    classes_to_ignore = [\"del\", \"nothing\", \"space\"]\n",
    "    index = 0\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        items = []\n",
    "        if class_name not in classes_to_ignore:\n",
    "            # Create a dir for new validated class images\n",
    "            new_class_path = os.path.join(new_dir, class_name)\n",
    "            os.makedirs(new_class_path, exist_ok=True)\n",
    "\n",
    "            class_index = ord(class_name) - 65\n",
    "            count = 0\n",
    "            for image_name in tqdm(os.listdir(os.path.join(dataset_path,class_name))):\n",
    "                image_path = os.path.join(dataset_path, class_name, image_name)\n",
    "                \n",
    "                # Validate image with mediapipe and brigthness\n",
    "                try:\n",
    "                    if average_brightness(image_path):\n",
    "                        result = mediapipe_validator(image_path)\n",
    "                        # if result is None:\n",
    "                        #     continue\n",
    "                        cropped_image, characteristic_vectors = result\n",
    "\n",
    "                        # Moving cropped image to new directory\n",
    "                        new_image_path = os.path.join(new_class_path, image_name)\n",
    "                        cv2.imwrite(new_image_path, cropped_image)\n",
    "\n",
    "                        data.append({\"image_path\": new_image_path, \"characteristic_vectors\": characteristic_vectors, \"class_name\": class_name, \"class_index\": class_index, \"index\":index})\n",
    "                        index+=1\n",
    "                        count+=1\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    # print(f\"Mediapipe didn't detect any hands {image_path}: {e}\")\n",
    "                \n",
    "            print(f\"Saved {count} images from class {class_name}\")\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8458 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8458/8458 [01:45<00:00, 80.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3213 images from class A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8309/8309 [02:06<00:00, 65.43it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4149 images from class B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8146/8146 [02:25<00:00, 56.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3971 images from class C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7629/7629 [02:27<00:00, 51.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4906 images from class D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7744/7744 [02:01<00:00, 63.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4049 images from class E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8031/8031 [02:07<00:00, 62.81it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4631 images from class F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7844/7844 [01:40<00:00, 77.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3273 images from class G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7906/7906 [01:52<00:00, 70.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3483 images from class H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7953/7953 [02:18<00:00, 57.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4352 images from class I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7503/7503 [02:00<00:00, 62.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3929 images from class J\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7876/7876 [02:25<00:00, 54.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4957 images from class K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7939/7939 [02:35<00:00, 51.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5446 images from class L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7900/7900 [01:55<00:00, 68.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2804 images from class M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7932/7932 [01:46<00:00, 74.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2480 images from class N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8140/8140 [02:33<00:00, 53.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4656 images from class O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7601/7601 [02:05<00:00, 60.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3751 images from class P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7954/7954 [02:12<00:00, 59.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4153 images from class Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8021/8021 [02:31<00:00, 52.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4873 images from class R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8109/8109 [02:25<00:00, 55.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4708 images from class S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8054/8054 [02:08<00:00, 62.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4037 images from class T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8023/8023 [02:23<00:00, 55.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4763 images from class U\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7597/7597 [01:59<00:00, 63.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4095 images from class V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7787/7787 [02:21<00:00, 55.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4728 images from class W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8093/8093 [02:10<00:00, 61.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3969 images from class X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8178/8178 [02:15<00:00, 60.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4506 images from class Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7410/7410 [02:03<00:00, 59.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4243 images from class Z\n"
     ]
    }
   ],
   "source": [
    "# Create csv and dir\n",
    "validate_dataset_with_mediapipe(train_pth, \n",
    "                                csv_file_path = r\"new_dataset/validated_train_csv.csv\",\n",
    "                                new_dir = r\"new_dataset/validated_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 49.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 24 images from class A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 43.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 23 images from class B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 42.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 22 images from class C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 33.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 images from class D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 33.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 images from class E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 38.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26 images from class F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 34.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 41.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 24 images from class H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 45.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 16 images from class I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class J\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 41.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 22 images from class K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 37.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26 images from class L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 37.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 27 images from class M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 37.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26 images from class N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 35.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 22 images from class O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 36.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 28 images from class P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 35.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 28 images from class Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 34.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 40.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 23 images from class S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 33.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 40.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 images from class U\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 35.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 28 images from class V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 35.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 29 images from class W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 41.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 images from class X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 39.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 27 images from class Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 34.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class Z\n"
     ]
    }
   ],
   "source": [
    "validate_dataset_with_mediapipe(test_pth, \n",
    "                                csv_file_path = r\"new_dataset/validated_test_csv.csv\",\n",
    "                                new_dir = r\"new_dataset/validated_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    # transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Values calculated from the ImageNet dataset\n",
    "])\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(self.csv_file)\n",
    "        self.classes = sorted(np.unique(self.df[\"class_index\"]))\n",
    "        self.class_names = sorted(np.unique(self.df[\"class_name\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        dataset_len = sum([len(os.listdir(self.root_dir + class_)) for class_ in self.class_names])\n",
    "        return dataset_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        \n",
    "        label = self.df.loc[index, \"class_index\"]\n",
    "        image_path = self.df.loc[index, \"image_path\"]\n",
    "\n",
    "        # Loading image with cv2\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        # height, width = image.shape[:2]\n",
    "        \n",
    "        # Converting to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        # Transform\n",
    "        if self.transform:\n",
    "            image = self.transform(Image.fromarray(image))\n",
    "\n",
    "            # # Przeskalowanie wektorów\n",
    "            # scale_x = 64 / width\n",
    "            # scale_y = 64 / height\n",
    "            # characteristic_vectors = self.df.loc[index, \"characteristic_vectors\"]\n",
    "            # scaled_vectors = characteristic_vectors * np.array([scale_x, scale_y, 1])\n",
    "        \n",
    "        return image, label#, scaled_vectors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset and dataloader initialization\n",
    "train_dataset = Dataset(root_dir=\"./new_dataset/validated_train/\",\n",
    "                        csv_file= \"./new_dataset/validated_train_csv.csv\",\n",
    "                        transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset len:\n",
      "108125\n",
      "\n",
      "Dataset classes:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\n",
      "Random images from dataset:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"Dataset len:\")\n",
    "print(len(train_dataset))\n",
    "\n",
    "print(\"\\nDataset classes:\")\n",
    "print(train_dataset.classes)\n",
    "\n",
    "print(\"\\nRandom images from dataset:\")\n",
    "img, label = train_dataset[random.randint(0, len(train_dataset)-1)]\n",
    "image = img.permute(1, 2, 0).numpy()  # [C, H, W] -> [H, W, C]\n",
    "\n",
    "# Denormalizing (there was normalization in transform)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "image = image * std + mean\n",
    "image = image.clip(0, 1)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(label)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset and dataloader initialization\n",
    "test_dataset = Dataset(root_dir=\"./new_dataset/validated_test/\",\n",
    "                        csv_file= \"./new_dataset/validated_test_csv.csv\",\n",
    "                        transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset len:\n",
      "674\n",
      "\n",
      "Dataset classes:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\n",
      "Random image from dataset:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"Dataset len:\")\n",
    "print(len(test_dataset))\n",
    "\n",
    "print(\"\\nDataset classes:\")\n",
    "print(test_dataset.classes)\n",
    "\n",
    "print(\"\\nRandom image from dataset:\")\n",
    "img, label = test_dataset[random.randint(0, len(test_dataset)-1)]\n",
    "image = img.permute(1, 2, 0).numpy()  # [C, H, W] -> [H, W, C]\n",
    "\n",
    "# Denormalizing (there was normalization in transform)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "image = image * std + mean\n",
    "image = image.clip(0, 1)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLITING DATA INTO TRAINING AND VALIDATION DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75688, 32437, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len = round(0.7 * len(train_dataset))\n",
    "val_len = len(train_dataset) - train_len\n",
    "train_len, val_len, train_len + val_len == len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_, val_dataset = random_split(train_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 75688\n",
      "Number of validation samples: 32437\n",
      "Number of train classes: 26\n",
      "Number of val classes: 26\n",
      "Number of test samples: 674\n"
     ]
    }
   ],
   "source": [
    "# Sanity check once more\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset_)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of train classes: {len(train_dataset_.dataset.classes)}\")\n",
    "print(f\"Number of val classes: {len(val_dataset.dataset.classes)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "Validation dataset classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset classes: {train_dataset.classes}\")\n",
    "print(f\"Validation dataset classes: {train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested num_workers: 24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_workers = os.cpu_count()\n",
    "print(f\"Suggested num_workers: {num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset_, \n",
    "                              batch_size=32, \n",
    "                              shuffle=True, \n",
    "                              num_workers=0)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=32,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=32, \n",
    "                             shuffle=False, \n",
    "                             num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "def loader_sanity_check(loader):\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        print(f\"Batch nr: {batch_idx}\")\n",
    "        print(f\"Batch size: {data.shape[0]}\")\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "        print(f\"Image shape: {data[0].shape}\")\n",
    "        print(f\"Classes: {labels}\")\n",
    "        print(f\"Num classes: {len(labels)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nr: 0\n",
      "Batch size: 32\n",
      "Data shape: torch.Size([32, 3, 64, 64])\n",
      "Image shape: torch.Size([3, 64, 64])\n",
      "Classes: tensor([15,  1,  3,  4, 19,  0, 11, 17,  7, 11, 17, 17, 15,  2,  3, 22,  5, 25,\n",
      "        25, 17,  7, 13, 15, 12,  0, 14, 10,  9,  8,  4, 23,  2])\n",
      "Num classes: 32\n"
     ]
    }
   ],
   "source": [
    "loader_sanity_check(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nr: 0\n",
      "Batch size: 32\n",
      "Data shape: torch.Size([32, 3, 64, 64])\n",
      "Image shape: torch.Size([3, 64, 64])\n",
      "Classes: tensor([15,  9, 21,  6, 18, 15, 20,  8, 22,  0,  1, 10, 10,  7, 18,  1, 17,  7,\n",
      "        13, 17, 15, 22,  7,  0,  8,  0, 16, 21, 11, 19,  5, 19])\n",
      "Num classes: 32\n"
     ]
    }
   ],
   "source": [
    "loader_sanity_check(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nr: 0\n",
      "Batch size: 32\n",
      "Data shape: torch.Size([32, 3, 64, 64])\n",
      "Image shape: torch.Size([3, 64, 64])\n",
      "Classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Num classes: 32\n"
     ]
    }
   ],
   "source": [
    "loader_sanity_check(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMET_ML SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : New validated dataset, Mobilenet test1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/ziemmi13/dlf-sign-letters-classification/fc6a9c0a60e7458faf682ebaef43da1d\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss : 9.718859672546387\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : New validated dataset, Mobilenet test1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size    : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate : 0.0001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_epochs    : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (4.58 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ziemmi13/dlf-sign-letters-classification/a1e98f8c21344b4c8541207dc7073dfa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "# Initialize Comet.ml experiment\n",
    "experiment = Experiment(\n",
    "    api_key=os.getenv(\"COMET_API_KEY\"),\n",
    "    project_name=\"DLF-sign_letters_classification\",\n",
    ")\n",
    "\n",
    "experiment.set_name(\"New validated dataset, Mobilenet test1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions to logs gradients and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.detach().numpy()\n",
    "\n",
    "\n",
    "def update_gradient_map(model, gradmap):\n",
    "    for name, layer in zip(model._modules, model.children()):\n",
    "        if \"activ\" in name:\n",
    "            continue\n",
    "\n",
    "        if not hasattr(layer, \"weight\"):\n",
    "            continue\n",
    "\n",
    "        wname = \"%s/%s.%s\" % (\"gradient\", name, \"weight\")\n",
    "        bname = \"%s/%s.%s\" % (\"gradient\", name, \"bias\")\n",
    "\n",
    "        gradmap.setdefault(wname, 0)\n",
    "        gradmap.setdefault(bname, 0)\n",
    "\n",
    "        gradmap[wname] += layer.weight.grad\n",
    "        gradmap[bname] += layer.bias.grad\n",
    "\n",
    "    return gradmap\n",
    "\n",
    "\n",
    "def log_gradients(gradmap, step):\n",
    "    for k, v in gradmap.items():\n",
    "        experiment.log_histogram_3d(to_numpy(v), name=k, step=step)\n",
    "\n",
    "\n",
    "def log_weights(model, step):\n",
    "    for name, layer in zip(model._modules, model.children()):\n",
    "        if \"activ\" in name:\n",
    "            continue\n",
    "\n",
    "        if not hasattr(layer, \"weight\"):\n",
    "            continue\n",
    "\n",
    "        wname = \"%s.%s\" % (name, \"weight\")\n",
    "        bname = \"%s.%s\" % (name, \"bias\")\n",
    "\n",
    "        experiment.log_histogram_3d(to_numpy(layer.weight), name=wname, step=step)\n",
    "        experiment.log_histogram_3d(to_numpy(layer.bias), name=bname, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRETRAINED MODEL (MOBILE_NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pretrained model (transfer learning)\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=weights).to(device)\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Ensure classifier requires grad\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Change classifier (fc) to satisfy aplication\n",
    "num_classes = len(train_dataset.classes)\n",
    "num_inputs = model.last_channel\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_inputs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(256, num_classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=26, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters: 3839490\n",
      "Frozen parameters: 2223872\n",
      "Trainable parameters: 1615618\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "print(f\"All parameters: {total_params}\")\n",
    "print(f\"Frozen parameters: {frozen_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4) # https://paperswithcode.com/method/weight-decay\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3) # https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "num_epochs = 3\n",
    "\n",
    "hyper_params = {\"batch_size\": batch_size, \"num_epochs\": num_epochs, \"learning_rate\": learning_rate}\n",
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, resume_path=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Load checkpoint if provided\n",
    "    start_epoch = 0\n",
    "    if resume_path:\n",
    "        checkpoint = torch.load(resume_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Starting Training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate batch loss (not divided by dataset size)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "           # Log every 50 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "                experiment.log_metric(\"train_loss\", loss.item(), step=batch_idx + len(train_loader) * batch_idx)\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = running_loss / len(train_loader)  # Average loss per batch\n",
    "        train_accuracy = 100. * train_correct / train_total\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Log epoch metrics\n",
    "        experiment.log_metric(\"epoch_train_loss\", epoch_loss, step=epoch)\n",
    "        experiment.log_metric(\"epoch_train_accuracy\", train_accuracy, step=epoch)\n",
    "\n",
    "        val_losses = []\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        print(\"Validating...\")\n",
    "        with torch.inference_mode():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        val_losses.append(val_epoch_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Training Loss: {epoch_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "        print(f'Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        print('-' * 60)\n",
    "\n",
    "        # Log validation metrics\n",
    "        experiment.log_metric(\"val_loss\", val_epoch_loss, step=epoch)\n",
    "        experiment.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = f\"./models/save_epoch_{epoch}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        model_path = f\"./models/New vaidated dataset MobileNet model, epoch {epoch}\"\n",
    "        weights_path = f\"./models/New vaidated dataset MobileNet weights, epoch {epoch}\"\n",
    "\n",
    "        torch.save(model, model_path)\n",
    "        torch.save(model.state_dict(), weights_path)\n",
    "\n",
    "        print(f\"Model checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "def test_model(device, model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    # Log test metrics\n",
    "    experiment.log_metric(\"test_loss\", test_loss)\n",
    "    experiment.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Epoch: 1, Batch: 0, Loss: 9.2058\n",
      "Epoch: 1, Batch: 100, Loss: 8.9996\n",
      "Epoch: 1, Batch: 200, Loss: 7.3834\n",
      "Epoch: 1, Batch: 300, Loss: 6.5608\n",
      "Epoch: 1, Batch: 400, Loss: 6.0558\n",
      "Epoch: 1, Batch: 500, Loss: 5.4208\n",
      "Epoch: 1, Batch: 600, Loss: 4.8655\n",
      "Epoch: 1, Batch: 700, Loss: 4.5407\n",
      "Epoch: 1, Batch: 800, Loss: 4.3952\n",
      "Epoch: 1, Batch: 900, Loss: 3.8147\n",
      "Epoch: 1, Batch: 1000, Loss: 3.6217\n",
      "Epoch: 1, Batch: 1100, Loss: 3.3520\n",
      "Epoch: 1, Batch: 1200, Loss: 3.4900\n",
      "Epoch: 1, Batch: 1300, Loss: 2.9089\n",
      "Epoch: 1, Batch: 1400, Loss: 2.7392\n",
      "Epoch: 1, Batch: 1500, Loss: 2.7593\n",
      "Epoch: 1, Batch: 1600, Loss: 2.1705\n",
      "Epoch: 1, Batch: 1700, Loss: 2.6217\n",
      "Epoch: 1, Batch: 1800, Loss: 2.5039\n",
      "Epoch: 1, Batch: 1900, Loss: 2.1294\n",
      "Epoch: 1, Batch: 2000, Loss: 2.2521\n",
      "Epoch: 1, Batch: 2100, Loss: 2.2361\n",
      "Epoch: 1, Batch: 2200, Loss: 2.3967\n",
      "Epoch: 1, Batch: 2300, Loss: 1.8356\n",
      "Validating...\n",
      "Epoch [1/3]\n",
      "Training Loss: 3.9967, Training Accuracy: 20.56%\n",
      "Validation Loss: 1.9715, Validation Accuracy: 51.94%\n",
      "------------------------------------------------------------\n",
      "Model checkpoint saved at: ./models/save_epoch_0.pt\n",
      "Epoch: 2, Batch: 0, Loss: 2.0709\n",
      "Epoch: 2, Batch: 100, Loss: 1.8454\n",
      "Epoch: 2, Batch: 200, Loss: 2.1466\n",
      "Epoch: 2, Batch: 300, Loss: 2.1778\n",
      "Epoch: 2, Batch: 400, Loss: 2.0684\n",
      "Epoch: 2, Batch: 500, Loss: 1.6084\n",
      "Epoch: 2, Batch: 600, Loss: 1.9021\n",
      "Epoch: 2, Batch: 700, Loss: 1.9082\n",
      "Epoch: 2, Batch: 800, Loss: 1.4895\n",
      "Epoch: 2, Batch: 900, Loss: 1.6198\n",
      "Epoch: 2, Batch: 1000, Loss: 1.8554\n",
      "Epoch: 2, Batch: 1100, Loss: 2.0669\n",
      "Epoch: 2, Batch: 1200, Loss: 1.7568\n",
      "Epoch: 2, Batch: 1300, Loss: 1.8630\n",
      "Epoch: 2, Batch: 1400, Loss: 1.6482\n",
      "Epoch: 2, Batch: 1500, Loss: 1.6936\n",
      "Epoch: 2, Batch: 1600, Loss: 1.7010\n",
      "Epoch: 2, Batch: 1700, Loss: 1.5006\n",
      "Epoch: 2, Batch: 1800, Loss: 1.3674\n",
      "Epoch: 2, Batch: 1900, Loss: 1.3300\n",
      "Epoch: 2, Batch: 2000, Loss: 1.6315\n",
      "Epoch: 2, Batch: 2100, Loss: 1.4191\n",
      "Epoch: 2, Batch: 2200, Loss: 1.3563\n",
      "Epoch: 2, Batch: 2300, Loss: 1.3197\n",
      "Validating...\n",
      "Epoch [2/3]\n",
      "Training Loss: 1.6544, Training Accuracy: 58.43%\n",
      "Validation Loss: 1.2189, Validation Accuracy: 71.94%\n",
      "------------------------------------------------------------\n",
      "Model checkpoint saved at: ./models/save_epoch_1.pt\n",
      "Epoch: 3, Batch: 0, Loss: 1.1204\n",
      "Epoch: 3, Batch: 100, Loss: 1.3396\n",
      "Epoch: 3, Batch: 200, Loss: 1.6441\n",
      "Epoch: 3, Batch: 300, Loss: 1.5359\n",
      "Epoch: 3, Batch: 400, Loss: 1.1051\n",
      "Epoch: 3, Batch: 500, Loss: 1.4554\n",
      "Epoch: 3, Batch: 600, Loss: 1.0422\n",
      "Epoch: 3, Batch: 700, Loss: 1.3761\n",
      "Epoch: 3, Batch: 800, Loss: 1.0615\n",
      "Epoch: 3, Batch: 900, Loss: 1.1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Due to connectivity issues, there's an error in processing the heartbeat. The experiment's status updates might be inaccurate until the connection issues are resolved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 1000, Loss: 1.4043\n",
      "Epoch: 3, Batch: 1100, Loss: 1.0910\n",
      "Epoch: 3, Batch: 1200, Loss: 1.2253\n",
      "Epoch: 3, Batch: 1300, Loss: 1.4265\n",
      "Epoch: 3, Batch: 1400, Loss: 1.2155\n",
      "Epoch: 3, Batch: 1500, Loss: 0.8954\n",
      "Epoch: 3, Batch: 1600, Loss: 1.0271\n",
      "Epoch: 3, Batch: 1700, Loss: 1.2099\n",
      "Epoch: 3, Batch: 1800, Loss: 0.8983\n",
      "Epoch: 3, Batch: 1900, Loss: 1.0882\n",
      "Epoch: 3, Batch: 2000, Loss: 1.0083\n",
      "Epoch: 3, Batch: 2100, Loss: 1.2282\n",
      "Epoch: 3, Batch: 2200, Loss: 0.8663\n",
      "Epoch: 3, Batch: 2300, Loss: 1.0330\n",
      "Validating...\n",
      "Epoch [3/3]\n",
      "Training Loss: 1.2333, Training Accuracy: 69.58%\n",
      "Validation Loss: 0.9495, Validation Accuracy: 79.18%\n",
      "------------------------------------------------------------\n",
      "Model checkpoint saved at: ./models/save_epoch_2.pt\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model(device, model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model and weights\n",
    "torch.save(model, \"./models/New vaidated dataset MobileNet test1\")\n",
    "torch.save(model.state_dict(), \"./models/New validated dataset MobileNet test1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the Model to Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "log_model(experiment, model, \"New validated dataset MobileNet test1 - sign letters classification model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : New validated dataset, Mobilenet test1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/ziemmi13/dlf-sign-letters-classification/a1e98f8c21344b4c8541207dc7073dfa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch_train_accuracy [3] : (20.563365394778565, 69.5843462636085)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch_train_loss [3]     : (1.2333032162332576, 3.9967047474390354)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [72]          : (0.8663419485092163, 9.205835342407227)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_accuracy [3]         : (51.94376791935136, 79.17501618522058)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [3]             : (0.9495286483912778, 1.9714663522483329)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : New validated dataset, Mobilenet test1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size    : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate : 0.0001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_epochs    : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (4.58 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element            : 2 (14.88 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 file(s), remaining 14.01 MB/14.88 MB\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 9.36 MB/14.88 MB, Throughput 316.48 KB/s, ETA ~31s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 4.47 MB/14.88 MB, Throughput 333.51 KB/s, ETA ~14s\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    # transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Values calculated from the ImageNet dataset\n",
    "])\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(self.csv_file)\n",
    "        self.classes = sorted(np.unique(self.df[\"class_index\"]))\n",
    "        self.class_names = sorted(np.unique(self.df[\"class_name\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        dataset_len = sum([len(os.listdir(self.root_dir + class_)) for class_ in self.class_names])\n",
    "        return 670\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        try:\n",
    "            # Use iloc for position-based indexing\n",
    "            label = self.df.iloc[index][\"class_index\"]\n",
    "            image_path = self.df.iloc[index][\"image_path\"]\n",
    "\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(image_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "            # Load image\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to read image at: {image_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Apply transform\n",
    "            if self.transform:\n",
    "                image = self.transform(Image.fromarray(image))\n",
    "\n",
    "            return image, label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data at index {index}: {e}\")\n",
    "            raise\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0780, Test Accuracy: 29.85%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = test_model(device, model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=26, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.load(f=\"./models/New vaidated dataset MobileNet model, epoch 2\", weights_only=False)\n",
    "loaded_model.to(device)\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 18\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Funkcja do przewidywania klasy pojedynczego obrazu\n",
    "def predict_image(image_path, model, device, class_names):\n",
    "    # Wczytanie i przetworzenie obrazu\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Rozmiar wejścia dla ResNet50\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalizacja ImageNet\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')  # Konwersja na RGB\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Dodanie wymiaru wsadowego\n",
    "\n",
    "    model.to(device)\n",
    "    # Przewidywanie\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "    # Zwracanie nazwy klasy\n",
    "    predicted_class = class_names[predicted.item()]\n",
    "    return predicted_class\n",
    "\n",
    "# Przykład użycia\n",
    "image_path = \"./dataset/test_images/y.png\"\n",
    "class_names = train_dataset.classes  # Zakładając, że masz listę klas\n",
    "predicted_class = predict_image(image_path, loaded_model, device, class_names)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=26, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.load(f=\"./models/New vaidated dataset MobileNet model, epoch 2\", weights_only=False)\n",
    "loaded_model.to(device)\n",
    "print(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "def test_model(device, model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model, device, class_names):\n",
    "\n",
    "    # image = img.convert('RGB')  # Konwersja na RGB\n",
    "    image_tensor = img.unsqueeze(0).to(device)  # Dodanie wymiaru wsadowego\n",
    "\n",
    "    model.to(device)\n",
    "    # Przewidywanie\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "    # Zwracanie nazwy klasy\n",
    "    predicted_class = class_names[predicted.item()]\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Inicjalizacja MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "class MediapipeHandCrop:\n",
    "    def __init__(self, max_num_hands=1, min_detection_confidence=0.5, include_characteristic_vectors=False):\n",
    "        self.include_characteristic_vectors = include_characteristic_vectors\n",
    "        self.mp_hands = mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=max_num_hands,\n",
    "            min_detection_confidence=min_detection_confidence\n",
    "        )\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Check if image is a path\n",
    "        if isinstance(image, str) and os.path.isfile(image):\n",
    "            image = Image.open(image)\n",
    "        # Obsługa obrazów typu PIL.Image\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image)  # Konwersja na numpy.ndarray\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            if image.shape[-1] == 3:  # Zakładamy obraz w formacie RGB\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            raise ValueError(\"Input should be a PIL.Image, numpy.ndarray, or a valid file path.\")\n",
    "\n",
    "        # Przetwarzanie obrazu za pomocą MediaPipe\n",
    "        results = self.mp_hands.process(image)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Get the bounding box around the hand\n",
    "                h, w, _ = image.shape\n",
    "                x_min, y_min = w, h\n",
    "                x_max, y_max = 0, 0\n",
    "\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    x_min = min(x_min, x)\n",
    "                    y_min = min(y_min, y)\n",
    "                    x_max = max(x_max, x)\n",
    "                    y_max = max(y_max, y)\n",
    "\n",
    "                # Calculate padding \n",
    "                total_pixels = h * w\n",
    "                # padding = int(0.0001 * total_pixels)\n",
    "                padding = int(min(h, w) * 0.03)  # 10% padding based on the smaller dimension\n",
    "\n",
    "                x_min = max(x_min - padding, 0)\n",
    "                y_min = max(y_min - padding, 0)\n",
    "                x_max = min(x_max + padding, w)\n",
    "                y_max = min(y_max + padding, h)\n",
    "\n",
    "                # Ensure the cropped image is a square\n",
    "                box_width = x_max - x_min\n",
    "                box_height = y_max - y_min\n",
    "                diff = abs(box_width - box_height)\n",
    "\n",
    "                if box_width > box_height:\n",
    "                    y_min = max(y_min - diff // 2, 0)\n",
    "                    y_max = min(y_max + diff // 2, h)\n",
    "                else:\n",
    "                    x_min = max(x_min - diff // 2, 0)\n",
    "                    x_max = min(x_max + diff // 2, w)\n",
    "\n",
    "                # Crop the hand region\n",
    "                cropped_hand = image[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "\n",
    "\n",
    "            # Wyodrębnianie charakterystycznych wektorów, jeśli wymagane\n",
    "            if self.include_characteristic_vectors:\n",
    "                characteristic_vectors = np.array(\n",
    "                    [[landmark.x, landmark.y] for landmark in hand_landmarks.landmark]\n",
    "                )\n",
    "                return cropped_hand[:, :, ::-1], characteristic_vectors\n",
    "\n",
    "            \n",
    "\n",
    "            print(f\"{cropped_hand = }\")\n",
    "            print(f\"{Image.fromarray(cropped_hand[:, :, ::-1]) = }\")\n",
    "\n",
    "            return Image.fromarray(cropped_hand[:, :, ::-1])\n",
    "        \n",
    "        # Zwracanie None, jeśli nie wykryto dłoni\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    MediapipeHandCrop(max_num_hands=1, min_detection_confidence=0.5),  \n",
    "    transforms.Resize((64,64)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class NewTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.csv_file = csv_file\n",
    "        self.df = pd.read_csv(self.csv_file)\n",
    "        self.classes = sorted([class_name for class_name in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, class_name))])\n",
    "\n",
    "        # Maping class names to numbers \n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.classes) if os.path.isdir(os.path.join(self.root_dir, class_name))}\n",
    "\n",
    "    def __len__(self):\n",
    "        # Zsumuj liczbę plików w podkatalogach\n",
    "        dataset_len = sum(\n",
    "            len(os.listdir(os.path.join(self.root_dir, class_)))\n",
    "            for class_ in self.classes\n",
    "            if os.path.isdir(os.path.join(self.root_dir, class_))\n",
    "        )\n",
    "        return dataset_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        \n",
    "        image_path = self.df.loc[idx, \"file_path\"]\n",
    "        \n",
    "        # Debuging label\n",
    "        # label = os.path.join(self.root_dir,\n",
    "        #                      self.df.loc[idx, 'label'])\n",
    "        label = self.class_to_idx[self.df.loc[idx, 'label']]\n",
    "                                                     \n",
    "         # Loading image with cv2\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        \n",
    "        print(f\"image w datasecie {image = }\")\n",
    "\n",
    "\n",
    "        # Converting to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # # Transform\n",
    "        if self.transform:\n",
    "            image = self.transform(Image.\n",
    "            fromarray(image))\n",
    "        \n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"./asl_test_dataset\"\n",
    "CSV_FILE = \"./asl_test_dataset/asl_dataset_csv.csv\"\n",
    "\n",
    "# Train dataset and dataloader initialization\n",
    "new_test_dataset = NewTestDataset(root_dir=ROOT_DIR, \n",
    "                        csv_file=CSV_FILE, \n",
    "                        transform=transform,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_predictions(model, test_dataset):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for i in range(10):\n",
    "        # Fetch image\n",
    "        img, label = test_dataset[random.randint(0, len(test_dataset)-1)]\n",
    "        image = img.permute(1, 2, 0).numpy()  # [C, H, W] -> [H, W, C]\n",
    "        class_names = test_dataset.classes\n",
    "\n",
    "        # Denormalizing (there was normalization in transform)\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        image = image * std + mean\n",
    "        image = image.clip(0, 1)\n",
    "\n",
    "        # Predicting image class with loaded model\n",
    "        model.to()\n",
    "        y_pred = predict_image(img, model, device, class_names)\n",
    "\n",
    "        label_name = class_names[label]\n",
    "\n",
    "        plt.subplot(2,5,i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"class: {label_name}\\n predicted class: {y_pred}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image w datasecie image = array([[[144, 140,  92],\n",
      "        [144, 140,  92],\n",
      "        [144, 140,  92],\n",
      "        ...,\n",
      "        [148, 144,  96],\n",
      "        [148, 144,  96],\n",
      "        [148, 144,  96]],\n",
      "\n",
      "       [[144, 140,  92],\n",
      "        [144, 140,  92],\n",
      "        [144, 140,  92],\n",
      "        ...,\n",
      "        [148, 144,  96],\n",
      "        [148, 144,  96],\n",
      "        [148, 144,  96]],\n",
      "\n",
      "       [[144, 140,  92],\n",
      "        [144, 140,  92],\n",
      "        [144, 140,  92],\n",
      "        ...,\n",
      "        [148, 144,  96],\n",
      "        [148, 144,  96],\n",
      "        [148, 144,  96]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 41,  29,  43],\n",
      "        [ 41,  29,  43],\n",
      "        [ 41,  29,  43],\n",
      "        ...,\n",
      "        [ 42,  29,  45],\n",
      "        [ 43,  30,  46],\n",
      "        [ 43,  31,  46]],\n",
      "\n",
      "       [[ 41,  29,  42],\n",
      "        [ 41,  29,  42],\n",
      "        [ 41,  29,  42],\n",
      "        ...,\n",
      "        [ 43,  30,  46],\n",
      "        [ 44,  32,  46],\n",
      "        [ 44,  32,  47]],\n",
      "\n",
      "       [[ 41,  29,  41],\n",
      "        [ 41,  29,  41],\n",
      "        [ 41,  29,  41],\n",
      "        ...,\n",
      "        [ 44,  32,  46],\n",
      "        [ 45,  32,  47],\n",
      "        [ 45,  33,  48]]], dtype=uint8)\n",
      "cropped_hand = array([[[151, 147, 100],\n",
      "        [151, 147, 100],\n",
      "        [151, 147, 100],\n",
      "        ...,\n",
      "        [ 43,  59,  85],\n",
      "        [ 40,  56,  85],\n",
      "        [ 42,  58,  88]],\n",
      "\n",
      "       [[151, 147, 100],\n",
      "        [151, 147, 100],\n",
      "        [151, 147, 100],\n",
      "        ...,\n",
      "        [ 43,  59,  85],\n",
      "        [ 40,  56,  85],\n",
      "        [ 43,  59,  90]],\n",
      "\n",
      "       [[151, 147, 100],\n",
      "        [151, 147, 100],\n",
      "        [151, 147, 100],\n",
      "        ...,\n",
      "        [ 41,  58,  85],\n",
      "        [ 40,  57,  86],\n",
      "        [ 46,  62,  93]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[154, 162, 186],\n",
      "        [155, 163, 187],\n",
      "        [156, 164, 188],\n",
      "        ...,\n",
      "        [ 23,  32,  46],\n",
      "        [ 23,  32,  47],\n",
      "        [ 24,  33,  48]],\n",
      "\n",
      "       [[154, 162, 186],\n",
      "        [155, 163, 187],\n",
      "        [156, 164, 188],\n",
      "        ...,\n",
      "        [ 23,  32,  46],\n",
      "        [ 22,  32,  47],\n",
      "        [ 23,  33,  48]],\n",
      "\n",
      "       [[154, 162, 186],\n",
      "        [156, 164, 188],\n",
      "        [157, 165, 189],\n",
      "        ...,\n",
      "        [ 22,  32,  46],\n",
      "        [ 22,  32,  47],\n",
      "        [ 22,  33,  48]]], dtype=uint8)\n",
      "Image.fromarray(cropped_hand[:, :, ::-1]) = <PIL.Image.Image image mode=RGB size=234x234 at 0x1CCA1C2C290>\n",
      "image w datasecie image = array([[[45, 38, 40],\n",
      "        [45, 38, 40],\n",
      "        [45, 38, 40],\n",
      "        ...,\n",
      "        [33, 27, 29],\n",
      "        [33, 27, 30],\n",
      "        [34, 28, 30]],\n",
      "\n",
      "       [[45, 38, 40],\n",
      "        [45, 38, 40],\n",
      "        [45, 38, 40],\n",
      "        ...,\n",
      "        [33, 27, 29],\n",
      "        [33, 27, 30],\n",
      "        [34, 28, 30]],\n",
      "\n",
      "       [[45, 38, 39],\n",
      "        [45, 38, 40],\n",
      "        [45, 38, 40],\n",
      "        ...,\n",
      "        [33, 27, 29],\n",
      "        [33, 28, 30],\n",
      "        [34, 28, 30]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[36, 30, 32],\n",
      "        [36, 30, 32],\n",
      "        [36, 30, 32],\n",
      "        ...,\n",
      "        [21, 17, 19],\n",
      "        [21, 17, 19],\n",
      "        [21, 17, 19]],\n",
      "\n",
      "       [[36, 30, 32],\n",
      "        [36, 30, 32],\n",
      "        [36, 30, 32],\n",
      "        ...,\n",
      "        [21, 17, 19],\n",
      "        [21, 17, 19],\n",
      "        [21, 17, 19]],\n",
      "\n",
      "       [[36, 30, 32],\n",
      "        [36, 30, 32],\n",
      "        [36, 30, 32],\n",
      "        ...,\n",
      "        [21, 17, 19],\n",
      "        [21, 17, 19],\n",
      "        [21, 17, 19]]], dtype=uint8)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_model_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_test_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[64], line 5\u001b[0m, in \u001b[0;36mvisualize_model_predictions\u001b[1;34m(model, test_dataset)\u001b[0m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Fetch image\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     img, label \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m     image \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# [C, H, W] -> [H, W, C]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     class_names \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mclasses\n",
      "Cell \u001b[1;32mIn[62], line 53\u001b[0m, in \u001b[0;36mNewTestDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# # Transform\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 53\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\functional.py:465\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m         )\n\u001b[1;32m--> 465\u001b[0m _, image_height, image_width \u001b[38;5;241m=\u001b[39m \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    467\u001b[0m     size \u001b[38;5;241m=\u001b[39m [size]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\functional.py:80\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     29\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected type <class 'NoneType'>"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_model_predictions(model=loaded_model,\n",
    "                            test_dataset=new_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(new_test_dataset, \n",
    "                             batch_size=32, \n",
    "                             shuffle=False, \n",
    "                             num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1393, Test Accuracy: 10.80%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_accuracy = test_model(device, loaded_model, test_dataloader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
