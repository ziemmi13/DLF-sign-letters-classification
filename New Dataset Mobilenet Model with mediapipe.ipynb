{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import lightning as pl\n",
    "import torchmetrics\n",
    "import comet_ml\n",
    "import os\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from comet_ml import Experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from mediapipe_handcrop import MediapipeHandCrop\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE TRAIN AND TEST CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_brightness(image_path):\n",
    "    # Wczytanie obrazu\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Sprawdzenie, czy obraz został poprawnie wczytany\n",
    "    if image is None:\n",
    "        print(\"Nie udało się wczytać obrazu.\")\n",
    "        return None\n",
    "\n",
    "    # Konwersja obrazu do przestrzeni barw HSV\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Ekstrakcja kanału jasności (V)\n",
    "    brightness_channel = hsv_image[:, :, 2]\n",
    "\n",
    "    # Obliczenie średniej jasności\n",
    "    average_brightness = np.mean(brightness_channel)\n",
    "\n",
    "    # print(f\"{average_brightness = }\")\n",
    "\n",
    "    return average_brightness > 135.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pth = r\"C:\\Users\\Hyperbook\\Desktop\\STUDIA\\DLF3\\DLF-sign_letters_classification\\new_dataset\\train\"\n",
    "test_pth = r\"C:\\Users\\Hyperbook\\Desktop\\STUDIA\\DLF3\\DLF-sign_letters_classification\\new_dataset\\test\"\n",
    "\n",
    "mediapipe_validator = MediapipeHandCrop(include_characteristic_vectors=True)\n",
    "\n",
    "def validate_dataset_with_mediapipe(dataset_path, csv_file_path, new_dir):\n",
    "    data = []\n",
    "    classes_to_ignore = [\"del\", \"nothing\", \"space\"]\n",
    "    index = 0\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        items = []\n",
    "        if class_name not in classes_to_ignore:\n",
    "            # Create a dir for new validated class images\n",
    "            new_class_path = os.path.join(new_dir, class_name)\n",
    "            os.makedirs(new_class_path, exist_ok=True)\n",
    "\n",
    "            class_index = ord(class_name) - 65\n",
    "            count = 0\n",
    "            for image_name in tqdm(os.listdir(os.path.join(dataset_path,class_name))):\n",
    "                image_path = os.path.join(dataset_path, class_name, image_name)\n",
    "                \n",
    "                # Validate image with mediapipe and brigthness\n",
    "                try:\n",
    "                    if average_brightness(image_path):\n",
    "                        result = mediapipe_validator(image_path)\n",
    "                        # if result is None:\n",
    "                        #     continue\n",
    "                        cropped_image, characteristic_vectors = result\n",
    "\n",
    "                        # Moving cropped image to new directory\n",
    "                        new_image_path = os.path.join(new_class_path, image_name)\n",
    "                        cv2.imwrite(new_image_path, cropped_image)\n",
    "\n",
    "                        data.append({\"image_path\": new_image_path, \"characteristic_vectors\": characteristic_vectors, \"class_name\": class_name, \"class_index\": class_index, \"index\":index})\n",
    "                        index+=1\n",
    "                        count+=1\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    # print(f\"Mediapipe didn't detect any hands {image_path}: {e}\")\n",
    "                \n",
    "            print(f\"Saved {count} images from class {class_name}\")\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8458 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8458/8458 [01:45<00:00, 80.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3213 images from class A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8309/8309 [02:06<00:00, 65.43it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4149 images from class B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8146/8146 [02:25<00:00, 56.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3971 images from class C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7629/7629 [02:27<00:00, 51.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4906 images from class D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7744/7744 [02:01<00:00, 63.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4049 images from class E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8031/8031 [02:07<00:00, 62.81it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4631 images from class F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7844/7844 [01:40<00:00, 77.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3273 images from class G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7906/7906 [01:52<00:00, 70.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3483 images from class H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7953/7953 [02:18<00:00, 57.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4352 images from class I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7503/7503 [02:00<00:00, 62.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3929 images from class J\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7876/7876 [02:25<00:00, 54.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4957 images from class K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7939/7939 [02:35<00:00, 51.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5446 images from class L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7900/7900 [01:55<00:00, 68.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2804 images from class M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7932/7932 [01:46<00:00, 74.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2480 images from class N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8140/8140 [02:33<00:00, 53.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4656 images from class O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7601/7601 [02:05<00:00, 60.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3751 images from class P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7954/7954 [02:12<00:00, 59.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4153 images from class Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8021/8021 [02:31<00:00, 52.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4873 images from class R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8109/8109 [02:25<00:00, 55.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4708 images from class S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8054/8054 [02:08<00:00, 62.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4037 images from class T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8023/8023 [02:23<00:00, 55.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4763 images from class U\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7597/7597 [01:59<00:00, 63.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4095 images from class V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7787/7787 [02:21<00:00, 55.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4728 images from class W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8093/8093 [02:10<00:00, 61.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3969 images from class X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8178/8178 [02:15<00:00, 60.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4506 images from class Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7410/7410 [02:03<00:00, 59.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4243 images from class Z\n"
     ]
    }
   ],
   "source": [
    "# Create csv and dir\n",
    "validate_dataset_with_mediapipe(train_pth, \n",
    "                                csv_file_path = r\"new_dataset/validated_train_csv.csv\",\n",
    "                                new_dir = r\"new_dataset/validated_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 49.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 24 images from class A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 43.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 23 images from class B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 42.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 22 images from class C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 33.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 images from class D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 33.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 images from class E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 38.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26 images from class F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 34.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 41.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 24 images from class H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 45.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 16 images from class I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class J\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 41.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 22 images from class K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 37.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26 images from class L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 37.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 27 images from class M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 37.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26 images from class N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 35.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 22 images from class O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 36.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 28 images from class P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 35.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 28 images from class Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 34.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 40.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 23 images from class S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 33.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 40.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 images from class U\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 35.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 28 images from class V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 35.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 29 images from class W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 41.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 images from class X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 39.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 27 images from class Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 34.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 images from class Z\n"
     ]
    }
   ],
   "source": [
    "validate_dataset_with_mediapipe(test_pth, \n",
    "                                csv_file_path = r\"new_dataset/validated_test_csv.csv\",\n",
    "                                new_dir = r\"new_dataset/validated_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    # transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Values calculated from the ImageNet dataset\n",
    "])\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(self.csv_file)\n",
    "        self.classes = sorted(np.unique(self.df[\"class_index\"]))\n",
    "        self.class_names = sorted(np.unique(self.df[\"class_name\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        dataset_len = sum([len(os.listdir(self.root_dir + class_)) for class_ in self.class_names])\n",
    "        return dataset_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        \n",
    "        label = self.df.loc[index, \"class_index\"]\n",
    "        image_path = self.df.loc[index, \"image_path\"]\n",
    "\n",
    "        # Loading image with cv2\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        # height, width = image.shape[:2]\n",
    "        \n",
    "        # Converting to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        # Transform\n",
    "        if self.transform:\n",
    "            image = self.transform(Image.fromarray(image))\n",
    "\n",
    "            # # Przeskalowanie wektorów\n",
    "            # scale_x = 64 / width\n",
    "            # scale_y = 64 / height\n",
    "            # characteristic_vectors = self.df.loc[index, \"characteristic_vectors\"]\n",
    "            # scaled_vectors = characteristic_vectors * np.array([scale_x, scale_y, 1])\n",
    "        \n",
    "        return image, label#, scaled_vectors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset and dataloader initialization\n",
    "train_dataset = Dataset(root_dir=\"./new_dataset/validated_train/\",\n",
    "                        csv_file= \"./new_dataset/validated_train_csv.csv\",\n",
    "                        transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset len:\n",
      "108125\n",
      "\n",
      "Dataset classes:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\n",
      "Random images from dataset:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"Dataset len:\")\n",
    "print(len(train_dataset))\n",
    "\n",
    "print(\"\\nDataset classes:\")\n",
    "print(train_dataset.classes)\n",
    "\n",
    "print(\"\\nRandom images from dataset:\")\n",
    "img, label = train_dataset[random.randint(0, len(train_dataset)-1)]\n",
    "image = img.permute(1, 2, 0).numpy()  # [C, H, W] -> [H, W, C]\n",
    "\n",
    "# Denormalizing (there was normalization in transform)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "image = image * std + mean\n",
    "image = image.clip(0, 1)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(label)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset and dataloader initialization\n",
    "test_dataset = Dataset(root_dir=\"./new_dataset/validated_test/\",\n",
    "                        csv_file= \"./new_dataset/validated_test_csv.csv\",\n",
    "                        transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset len:\n",
      "674\n",
      "\n",
      "Dataset classes:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "\n",
      "Random image from dataset:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAERCAYAAABSGLrIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvsElEQVR4nO19W48tV7ld1ap17e69e9+3DZiTA+KgAEIgEMRRIhyREDg+HCXiFgUQb5EQCBDKvwAekHhBykseeAGivJArcCIEFpbB4aDImBgCNrbxvu/du7vXvaqitRr3HN+Ya85Vq42x2HOMl11zz7rMmqtm1zfq+77x5XVd15kgCPc0Wq/0AARBePmhhS4ICUALXRASgBa6ICQALXRBSABa6IKQALTQBSEBaKELQgLQQheEBKCFLggJQAv9HsNPfvKT7LOf/Wz25je/Odve3s5e+9rXZh/96Eezp556ytv3ySefzN7//vdnOzs72blz57JPfvKT2fXr11+RcQsvL3LFut9b+PCHP5w98sgj2Uc+8pHsrW99a3blypXsa1/7WnZwcJA9+uij2Vve8pblfs8991z29re/Pdvd3c0+97nPLfu//OUvL/8wPPbYY1m3232lb0X4Y2Kx0IV7B4888kg9mUzM/z311FN1r9erP/7xjx//36c//el6MBjUzzzzzPH/ffe731380a+//vWv/0nHLLz80Bs9EbzjHe9Y/vv4448v/718+XL2nve8J/vmN79p9nvjG9+YPfDAA9n3vve9V2ScwssDcfQEsPhbfvXq1ezChQvL9vPPP59du3Yte+c73+nt+653vSv72c9+9gqMUng5oYWeAL7xjW8sF/fHPvaxZfuFF15Y/nv//fd7+y7+79atW9lkMvmTj1N4+aCFfo/jl7/8ZfaZz3wme/DBB7NPfepTy/8bjUbLf3u9nrd/v983+wj3BrTQ72Esvrg//PDDyy/r3/72t7OiKJb/PxgMlv+uemuPx2Ozj3BvoP1KD0B4ebC3t5d94AMfyO7cuZP98Ic/zF71qlcd971osr9owiMW/7fwqa962wt/vtBCvwexeCt/8IMfXAbJLL6ev+lNbzL9r371q7OLFy9mP/3pT71jFz70t73tbX/C0Qp/Csh0v8dQluXyo9uPf/zj7Fvf+taSm6/Chz70oew73/lO9uyzzx7/3/e///3lH4dFsI1wb0F+9HsMX/jCF7KvfvWryzf6IvSV8YlPfGL572KBLyLjzpw5k33+859fRsZ96Utfyl7zmtcsw2hlut9b0EK/x/DQQw9lP/jBD4L9+HM/8cQT2Re/+MXsRz/60TLkdfHh7itf+coymEa4t6CFLggJQBxdEBKAFrogJAAtdEFIAFrogpAAtNAFIQFooQtCAtBCF4QE0DjW/Rlq55XbriJ/LlqZddMfXr9q2v/jP/4Ht+/t2/bYIodte5HD8dS0n/69S9CY1zC4LMuKVuGN6kUcjIamZ3vbZW2dO3Xa9J3f2TLts9DepkiydqeDA7BXb9l2G9ovZpitareoL8+bh0C4mcyyChsL1Pwfrp3PZqbnNz/7uWnffM6F0E7ndt/hvHTnGdj5efs/erdpn7/vPjc+Hg2EeixCfBH+s+fGXuT2vmo4D1+jrMrob2T2Le2853BNDkupsM2/lzfvZrDBsTP+1b/799k66I0uCAlAC10QEoAWuiAkgBPno9eRPxHIJioiQ0UXuGuWZR3QD58TnzdckXqKnHgv7FBXdZR/FRH+FQNRPsObmEJhH3LMVZNSw4mjqQfM27z5wrESPw2M7Wh8dl9sTYdWUuoWFXi4u79/vD0HTr7AEPj9bP/A9D379O9M+/zl+4JcFu/T+77AP0oWfg4qcxzv7P2A4Wvwrjg+Pk+EhuMze3QsHhf+TU4CvdEFIQFooQtCAmhsusccORX11hGTO+/YUj8dcEvNaO8czuuZ7mT3oKnqma0nTMRlEzdm3THyyF9THh+6Z7iPrmhbRAHw2Pgtx6lEBrRoOrVuzMkfxCOP+2dz1ze17rXp1PUNS7e9wI3rN+2I0G1G7sgKbNoW0YzK51Ouz7exIwj/Jt5xG9ArbNfk1KuIftrjIq7BE9jxeqMLQgLQQheEBKCFLggJoDFHt44Ty58ZxpVDf0rylr1kd+DCSA+YE1fhcE+fo7sL1fV8DSfGvuwlIOwWM5yK+jg086SDyDc4D86PHwAadkuVwMEXmBOfL+HeZuTOmkKoKIeNjg6t2246ctx/xt8M4Br9wVElmRfRIp6LzynPRgvmh/k79jFKGk/lfdOIhbLibp5PL3xNzyULXF8cXRCEVdBCF4QEcPLIONwmk602ZjWZzZSB1dvaihiUeGw4Y8jL+rIeIS/zDU2tmDvLj1Ij145xndCxwaNWXil4TdsOu1zWXcf8Xl4nnQfmZE5usYqiDNGUZ4pSAvdCl+sCB3fvmvb/fvSx4+0Z0QWMbHztX/yF6XvgDa+z99IGCkf0rsIsODa/CeaJWeeejUXRhfY72pkuCtmaRFXRjRijzSHojS4ICUALXRASgBa6ICSAPwpH36SPOfFgexs7aefwOWNhpD4Tau6PiGWP0acIcnNEuH5E6WTZjowHXTnx8Fh7Ji8MGM7jZR5691wFM//4NquyCruh4Lwt5uh39kz7V088eby9BS7X5Xlg4m/duGH6tk7vmPb517jy0GVNqjEZcuANfFSRTMBld+Q+TbjqJlzfC6XFLnF0QRBWQAtdEBKAFrogJIATh8AassZSGRFUpO7S3QaOReqtKPHJPlH+E9UGpdd1/CumzIJ9vF9Nvk3Dg8PZkmvTVJGAcQoihvp6fvNYSKXvLHebHGHKO0PKaEXKrjko8y7Q7sK8E0fH2AaKgM3mY1bqde0ZpLcyxndtkMTvnrb6xOdfdV/ksazxTkxP9PsHP5fcxgt5Sq8Nfexr8FKLHuuNLggJQAtdEBLAid1r1hQkl0JovxUcoNvfCovmg1vFq1VAZhAKPnquN89t12ok/MdBkrHw1Gj2GrigViJ3P0MRcaswPfDvMzxWPNjP+GJK4LYrKpjQ7thHpt1zYp9tnoOW65vMrTk+G9rzdvNeeDzQLGkur165Zu9l4qhG0WuHM8Jy0xWnVx5lomMjJnmsryIKYKOd6bfdgB6vHMdLOloQhD8LaKELQgLQQheEBNCco8dophcbGtmX5DG6PVfUsFXY4g6tLOxm8dRDjBuKQzGp2IONJ6R9IVSUORS72yIuj7i7htRNwnuSqinBU4Ft6oJhhRTuDyuxzHhO2pA+CSmiyz54Lir6KTGFdbkvjKHktGcM36W+w4ND0x4euqKZp7q7WQitNa4uDOflwo6buLq8gg4nPk4cXRCENdBCF4QEsIHpHsvqCkdoed4ishq7fZe91mpb0z0Hk8lLuCJTBk33dYhGxsUSiKgds/7QdOexxWpve0MzqiPr7hGVc7LG8OqRw3ZJNvdsbiPTUIGGzfwSzjyZTYLHHY03D5rKsSpkk7E97/6eU645dXY3KihqQb8RUolIkYjlkU1r53kIu0dbzZdfI+iNLggJQAtdEBKAFrogJIDGHJ15uOGyXvE/VHshPuPVSx8EFWJt9Kf9m8TtDobAesX1opo3kb41e4YjKunAeEiu4fPRkMnmRQdi2Wt8WBGZr+nEcuB2YR+ZXs8VVMjb9sedZ5j5NoqOHdueaxJS3/jNVFJN9slk3Ijntta51zbg2jFeHrsvVuQx56FTctjyptAbXRASgBa6ICQALXRBSADNFWZIIgSVVf3QzPB52P9ddLpuu2v96ETrgtdfHgv8nvv8WoRhfzOGWzK3j6m3vhQ3Z9T3GlEoYc5HAahZU0ldjpzFaiwTKH64QDm1ijP4DYbnHflpQdVycoolwPBd/o4TuIuV7Tlw9njUdh0c67IfCL3H5+lbUsyPjtfximtmL38o7YvQG10QEoAWuiAkgMamu1dIEbY9RRDY9lwcTAFAsaTo9Oy+EdHENrsfIuGNvinfzOjmnkjJao+SVHANv15CeL78Wupo+pHJ6MXk+vewss8L4bQjRFWZ2egwarpPStcuaQDlvA66wfzJhRBYzj4EVRm/YCZTAnMkXSR35+FCFLEf+yUkjqHZ7xV2jITW+tRC7jVBENZAC10QEoAWuiAkgOYhsJ6vIpyKin3M25gM5ZCaWnQtRzdczasMWDVPU41w4jxS1NC42lYK6WCxB74k9q1RCzGfDGL7xl0sMRVR7GRuj98TFphOXSpqSeml3vcG4N7TuZ2vWbn6nKuA7jbfDZUF+Tt/XzBqqZ4vro6kfYZVhfkbVAxecctoAYewK44h95ogCGuhhS4ICUALXRASQHMpqQhF8JhHTPeG0l3zzPmGO12X8rjANMJPufpJ21RqeQkVVqLHedUJw4hcgw+06bg8+Fh8AF8Sw1HX7By55hz49JwqrDBfLSM8EsNISzrPoO9Cnxfo9TvBaiwFPKZz6uNPN978mZ0zt8nfiiIKuhwC6/nDT4im6a38LeIk19cbXRASgBa6ICSADUJg8w1MkPDfD09xJuusLLi4wIHJjGJTi64IpruXibQmC830gVmErraj48KhmWxOdWrIpvM8ZmSWofKJNyC0N8l1481JQ7OV++i4CajKsNKKF1SK806103NQeq1rKqrYs6Y7lLfP6jwcShujYcvxMb2ye2cnAZ+RnwuDjeR3Yxl0EbpwAleb3uiCkAC00AUhAWihC0ICaM7RvfBPBHFHQ2EiIYHkDmn3rXsN+WHHk8UMp/G1mN943LoMcz4MgfUK1Yf5IIfL1lUrfBzvG1HrKZD3RlyBC8RqLNYm/JPTW+1Vp1D9hNNLeS4zw9Ej4yPq2ulajo6fHzxObEKN6RqcYmt+v3CIaYuu4T2XqBqzzp0VUfHFa649jymoyc9leKxNoDe6ICQALXRBSAAbZK81j3aLhWFVXsk897emv7UTPK1n7nqRQ3jOeFE8o5NIfXMw6zlCyxPchzmJmfVVSaKJ5IYyAoJ0Hq4VjuAsr2jxSPiTzgZki645GTtByBKEIpfHMj9AxRfPFRdWaSlAWWh5HTgWBR6PxgBzuabypY2cY7HK2p1nnaIpPHwolrnsoiONQlDMXbvG5I4VlagxBvEEijd6owtCAtBCF4QEoIUuCAlgg+w1dp1gUUNv57ByByvDTIfH29OxrdgwAa7G4p8FuVVM5tYaio7RvPztAbOsOHNrOrMKqPMK9gU11AXarUZCJ3+4GWpjF97MBgUlPLqHN801DGdW/aWq3L3k9D2hS26xGczflOZrhhIz/G2GFIHwGwy7loyLyivGYdsz+I14nusshvBD42W60cOI4dclfONZjh0/BmwSHhv5FnGSQF690QUhAWihC0IC0EIXhATQnKN7QKIbU1Ah3/Pojmk/+bNHj7d/9N//i+k713HHFls2PLbXoqHnYSo0IQ6KYZMz4lTjieN4Y1IunRBHnwEnZcWbMi+DqqtUcCXuFsUUViruV0VURb1KJHDPzHNZ6XU2GQUro3S6dgzlyM1Bzmm05rkgXutXvsSG3bdoB+Mp/NRcjG0gvpzFEE4LXceszfeFDdRf/MKOzc4Ti5cIXmvjIwRB+LODFrogJIANTHcSysdsGm9f9z+He9dMz6P/9T+Z9uM//F/H2zef/53pe8sbXne8vUW10/OijLprEBzKWoERx/W/9/cPVor4L9Dv2DFMBoPj7WnXupZwOBWozSzHw54TMNNaHHKKY+DCe1xjHAtneD49UMOhX2w+ci7OBSbQrpmSGJfZojCDozOzKbkjJ476tOn34bktQY3GF3iE7DA6jjkKKt7EorYzX1kzsm/ceG8s1rhGraex2a/sNUEQVkELXRASgBa6ICSA5hydVWBN8TjmDI5r/PYXPzc9j/3dfzPtvZuOw5cURnowcm6e3dlp01cUls90O60gj+sStz4AXn5weGj6OuDKwXTNBfb375r2DXA9UaRoNi17K8+5ql2Cy6ogPl/ANebkLvJF/mGbXVRmTuzcjYZ2DiZDN+8lcPBVPBzb8ymll4I7cmfHKvy22VUIijyoAHTUByms/J2C5wDuO1b8M1tHcyPFET2FlxMWQPRUiRqe5yQFF/VGF4QEoIUuCAngjxIZ55kSU+eieuZJa7qP7t4OulUmsL3AHpiQ5yhKrccRW0VvpVAkDXWJIZiqWyRIubPlTMydLec+W+47sPXbe5DJhZl2C8zIZWXGStSiDaZ8t2NN2l7PXbNbhM16jhJj070ANxRdIhsdut+LTW7fVGf1F9xmRR637+mdU9E5qLHdYVMZ7mONi6oDyjW+u6rOwogUTNigrjlH6hmRyXUiobHCIigcSS7YJtAbXRASgBa6ICQALXRBSAAbFHAIa19ywYTb168ebz/91C9M32hMaiYgrzKhLKo7dx13PDhrXUADcKctx1AW4awuGvqg73jvhQsXTN/Z06eD3wE4C82mzEWyjShMk11LmOV1SCG5E3BvIc9e5UZsQb/nYiyci3G7b68/JmWfyrjxmC9zcYzVCrpH43XX6VNRxZq+x2C8KrsqUTmYFWs5Ew/nFr81cBELDJX9Q2eQPzPvjtQbjRaCWLdvc2wuA6s3uiAkAC10QUgAWuiCkABOrAJreAoV3nv21//3ePvGlRdM3zxSiWRMIae3gCdd27I+7II48SlToIO5LPuUHY8b9Kwf/dyZM8fbp7etH73NaaGmlAylaM4dPzyEUN6j4dnxoIIsU0esZMOVWTIKiR1jcUTy0aJ6a122o+q2+HPOKddz7vnK65XfE5bXBJ82ptCuCnO1qDdQzsmCPHxKIdWheV22aW5RzZWVfbyAb5gwPg9+K2E12U2odr1JscYV0BtdEBKAFrogJIANQmDDBf2qsXV9/faXTxxvD/dtHxftG4E7iYs0tOE/9u7eDYbOLrDd3z/e3trZtiMnc2oL3GsHlJH2KwgHvXThnOm7eHrXtHchI2ubQmmLNozh3Fl7Y2y6g9nPYpWHd10o7eFdd4+rTOUaRDC7XUt1Om33U3dIrWd/bM8znrjxjOgaUwqJRSuSsw+RLlREbWp6Dkp453ABTXzW5hz+2e4EXXFjCtfNzWkpEzBijhdETT0nK4YexzLdqrhrEPdtAb3kPmWvCYKwElrogpAAtNAFIQE05ujsDEGacHjHhbwu8MIz/+94ezK1Ya2s2tJtOy5y/8VXm77TpxzP3aKU0RbRlMnEnfcuKLkusLtL6jTAVzlcFtVWWjftNWpyQ5WzHXee7R17X51ukBN3qFDhoO3a5diOfe/6jePtOaXqzmhujVIMVYl44PX/4Hi7B2Nb4BDcckftaZCjz0gVdjqrgvx9B75bsBIvE90SEk6rWEq0d5o6WGTj8NCmCucQW8shsCySZNR6clL4pWMxXdgLeY2Ky/K3CBgfqwFDm7Owm0BvdEFIAFrogpAAmmevUTsHY/65X9sMtavP/fZ4e0RRYajgssCFc+dd3441fweg6NIHl9gC0wmbm+46bTKVeexGgYZDrUyNMvt3MKZX4usQYh0ta/pxwNZs7Pb99S+fCkbjDbrWhbd31RbHGA/HQRfM6LxzI57aumT6hmS6jyCqb0q2Mp8Xa6Jz9loss82rVYe/A7mWTG1w7qN31XDk7qXO94NRdCW5zDgDE12DbGKjIhAXoyjYrsZHzRt7FlEIotPA/HharA2gN7ogJAAtdEFIAFrogpAANgiBpewjcGf9/PHHTN/NW84vdeG8DSO9dOmiaQ+gUCGGaS7Q67r2FK63wO3bt4PckcM/PdcJ8B2P7uTN+jxEwhJbXm1w296742rG392z9eMvnXfhs9euPG/6DvZs+O4EQ1npGs/8xhWwPCQX54TDWoEhtqn4RU1+zRzqyfP0oCIOuo4WmBNHLucR15eJjaZMRMosQyUbdKPytxJWycVsNf62tE41Bsfrc2vg78TtvSKZcF5WIWoDv19T83El9EYXhASghS4ICUALXRASQGOOzrSgmjqeN9q3/soH/vL1x9vnzjrFllUFDw1PId42ggKIt29ct9cH/y1zSU+t1WtXkZRD+g/ba8eOfk/ykSIf4z72Rc/Asc6VWiaQNjum0N5qzmoveZADH95y3H9vMoyq9eAkcPgucsUlgIfv7tpqLNtQZaZFM51Pyf8NYbecgtyGx7TVJnVbGt95/CZEec8jSImeUmWduiA/v6mMQvNMcxs6ju/FSytm1SZo87cHfNZYMakJ9EYXhASghS4ICaCx6d6urNkx3Hem4P333Wf6Tm07k60up/Ga0GDS7d+x7qK9mzeDZmpBf6NQ9H/3lA2lnVC4LBrsHLKIpjtnRkWLWJA5ZcIZWTqHw0ihcAXvOgU3T0XZc9WUwlOhOSPzsj1AtRfbx8KNrRaYyjQ/bcg2XOD0KRfSXMBxy/OCoko1mwcFOhfoAPUag1LO8l7mrl1kfJy95qULLqS63bNmfQkxuTMaz5QzA4Eajuj5GVMbVW9KopRzoAgVuSY9sUgca8SsL2ktNIHe6IKQALTQBSEBaKELQgJozNFH113q6QLP/topvY5GVul1CtyRhfIzcp3s3bl1vH33plNTWcIocdLAyf3wmouOm53esW6e3125Gvz75qt8ZOHQR+bs2I766ega5M4qITSzohPNwCXDYZplTYo30N2i8M82uKXWuYt6htsSVyReiWGu7NasIRWV01K9uYWPLMzDsdiDl3JMYaQVPl8lhctmEI4K4dUL9InPm1Bouigr9c4hrRe5Pf9+44n9DjChgqMzVANmNSNMe2a1ngbQG10QEoAWuiAkgMam+42nnzTt8sC51+bsvopYtHu3bXbWnRvOdGflv7zlTKQeFc1+7eXLpv3qc67O+YzMS4wqWo49MNblNY3Jxi4OqpMNfye5ylcb/4ZSgTA+7xzcJSyiiGKHLPjfpZrjyG+sUbhQtZkG/7xjjbTl2Cl60ZzHEy2sg3053IupU7ciewztY+7LwcVXkHuvTRQF3YElRb+VxkW8JroMzsP0gMMncQwoCrpA1Xf3dYYVi+jZm4PZPwM6wLX82BXYBHqjC0IC0EIXhASghS4ICaC5wsyMiiVOXQZUTXW60eVy87rNOrt1/Yo9L/IoKjowGLhQ1vvOWpfZ/VS4sAc8qeba2yQxg26qNtezzsKqKCW5feYZKIV2rEIr1hXfu2MrQYyGNtT3JoT6zjgUE3gvh+SyEgu60JDvcQhlh5RO2sTRMfOOXW9+CHMWrMlewW/b4sILnusyXKgwdhwqsC6PxVBkVrwpisYcHe+zJL7s3SeS7Tw8Hub6XiEIzFCj3wSVc3JSS24CvdEFIQFooQtCAtBCF4QE0Jijz6m8SI2hj0R3rl75/fH2resUfkpVS5CPDXatb/y97/trd/1rTsV0ge7I+uM7oMxir2CrnXD1EZ8rhludgSv6uMCl+11RyINbtmrK1efceMdDqwxTTmz1muFwP5g+2QHOx2GRXU57xBBP+i5huCuroJC/GVVjclYjJX4/h/HOPeUcKAxIqZX8hsHvDb6AC/RRDyux2IexPpFqLz+XfA3+NoIhqd53HfjGMufvG5ExrFOe3RR6owtCAtBCF4QE0Nh055raWe3M0d8/96zpunkNzHW2w0iFpLvjxPz+2d/+W9P31ne++3j7yb/7z/Y05UEwm63vCTWS6QWmFiuomOMo3PLyRRdmu8A+uM1+9ytbHHG0v7c6o2pp3tk5mUGN8f2DcFHKKYXHovIKu81YvQSbLTYL2YWGjYIz7SisdF4HCymii5Fdnh36jaxbk0Ohw663mtpGuYat3RrGk8VhRB65KDvX5QRZoIgOqedO28R0x3bsuBD0RheEBKCFLggJQAtdEBJAY44+hmIKC/yfv//74+0bV14wfUZphMJai/6uaT/08L853n7Lg//c9M0hbrKz5YoxLk+7Fw4f5DBEVhzNcyi2x3wLCFi/Z69ZUuGDm8+DC+0uFX2EUNaKvwPQN4MKQ06JW0/BfZW3bfroeDgM8kr+9lABl86Jc7YzVlcBxRvyvHnhn3UsNDRS6IDn3QyXuCxwdkxZXf2NJexeq6EPC0l6h1Hbc23xpEQ8egWGsvJ3k8j88HjwPrlIRBPojS4ICUALXRASgBa6ICSAxhz9f373+6b9zDPPHG+zKCUWFewMbJHFh/72E6b9zoceDqZ6ZpUritft2fDTGXF/zCClrNmsxX50w9XYXwmpguT7Hd6wYa7zAxe62uESK5AyyjJKJVdGgfGw7/5g6L6NbA2cT305dvLLjkBxlOMeUE6LQxtYvgoJYk23VZGPG+W1WBoJ9+TzYHWaBYqyCIaY4pzUns8/Uv2Ex17Vwco6rG6LP1m1RlIM/doFPwdItilkmZ/LCrh/jIdTdHMj6I0uCAlAC10QEkBj0/03v33atDEZiT0TvS2nBvNP//rDpu/dD33A7twZhBVU4O9Qu9ePmu45FEVok/nEiiomTtHzr6HNZk33EdQqXwIy+sjizmqwVbkmHmc4YWhkm1xoe3tOjaagPm5Pp84tNoTijAt0u72VCkDL25iSeiuqy+br1FsxKy6WnWWP29reDoYwY2HCZRuy9lpUP57NfMy8i5m/lRdiyqG+deNim2j2s+INntc/Dw0KC3NSF17zJJlseqMLQgLQQheEBKCFLggJoDFHx3TEZRtDRbcprPVvXFjrg//iX9sTdayLyMBT4IDDiKMPmZsB/ymI4fQoNdYeRSmR4LvY6dM1b9hQ31Y5CXIz5OysvMLuLUzD5Moj6LPa33PuvAX6fQrRhX0no7HtgxDmrZ7j6wucPrUdnMuaYlWNUg2HO3s5mq7tBXt6aqngjqTUWIyX9WgtjSeKHFynxHPZZRYukenfCxbf5FRd5OXr0kuRe8d4OBfbbAK90QUhAWihC0ICaGy6s7nS7jtz78F/+SHT9+D7XLsgU90z4cqw6ge613JwDy33ZeF+/8zHaHnRSngie1U0a3ttaz4NSSBzBsUlW1Tw0BQ6IDcdut6Oxh4W+W933XlvYkHKpcJMuJjkhFxUUzDld8i11eNijTBeT9GFC2FGxp5BhGRN45lQrfByDuZ5Ho7c89xpBDNezgCroweaZrH6EQ1QlGAjarrHTPmY6R7NegtAb3RBSABa6IKQALTQBSEBNC+yOLBFDf/xe11xhYcetmGuOWSa1URwmEubIn10Scz6anGBeeZqhvcS9+GwRMxiIi506pT7pjDct8UQD/ate2s+cby31Tptx57HOJ29U0u57L5d4OglzeXBwWEwJJZdMJOZO3ZIrrdzZ+zYO5gtlsXdUJj9x8qzWeS3LTy+GuHLMB7mru02fV/Awgs8nFaY9/J4UC2Hopu98ZkoYN4VvY/ecHhuwR0ZKfYg95ogCCuhhS4ICUALXRASQGOO/l6qovJP3vc37iT9M4194z7jAb7h+SvhqA750Vn9E4gSVs5YHks5pCa8kXhlH6udUDhqh9JC796+E1Qj3d6B7xTs92SFF+jn9MkOqIh2aDx7+1YFtmjDXNJ4puAbv7tvuX112Q6o6KJcD3NXHjvuGua5fB5WdEGFnhap9iIn5lTPrYENA0bEC2jG90X+XK0t7AjnoX2Ra/NbteK0XvOcxEarNFVBEFZAC10QEkBj0/2hD1gXWgZCjn7oKtYf5z42ubOwaQOdeWFDaXMWh8wgpJLcFj0y3SPCJ1k9dsoswzu2BjvfaLfrzMaDQ+uy6nb7wdhLT9wPqY5n5ru+zsC6ksoDa7pj5lROLjxUOik6VATBqy8B/8GiiWRu2qkmCoBiPezi9AoVhsUh0VxnhZsBhfNidxVVUcyjTe93iAEnYYOwVhYNbTNlAZQw7yVXv2gAvdEFIQFooQtCAtBCF4QE0Jij56DW6qtS2n2RpfiUhblPhDAbjt6NF2VAFRL+LsAhi8gzietPIax1Nra8ewZqpJw+OR7ZtMvxwKWwdtBdtaLQwRzUW6eUzlmChGynsO69Ttf+fDMoyMjTfAa47A5xfZa8aUPxiYrGw7yyhcULyKVXgjJvi3NE6UeawtwWFX1DgG8GGBK8wKnTO/a8Jk2VH8waNtdUQYiovXAxyei5jGtwzXO5whm9at7bUoEVBGEVtNAFIQE0Nt1jhg67ImKWDNcd8+xY0wXuoo41W1uctTQbBetfdelYY/mQC2YCpvu8BFN4Ta0HnoODQ+f6OpVb1yBnoWHBgpJM5dksHDnY4QgtOO/Oto0kvHwWohdrS0FYgMeIFHInF7aLeaHQrOfIuMi+XnYWzPNgy87l9vZW46IRCN+qJxfoJuZxJHIvVnjBKzBh6uOxoCjSTZnugiCsgBa6ICQALXRBSADNFWbWFKVrfJpNesFdk1MRBo+jR9Q/25ABdrQrqJB4arJwDVIk4XvGbwic0YQqpz1yCTEHnYOaKxZhWF4Tw2PpuBbN1+7A8fLLFy+Yvp2e+05BXsKszSHC2CCOXpOibcybhRlgZSQ7jMFzgM/euQvnTFeH5jZWWDGLXDM/YVjrcld8Tta4dqNxwFmkJrsZzhrX4ArojS4ICUALXRASgBa6ICSA5hydEVG3xP9hvUo/zC+iRoOcmH3GpPZi1ErpRG06tgtKLeyLLueQ+kmcvJxbv3ob/PNt67bOZkN3njFUdFneCn1vqCCUtkUhuRiO2p5RH3mjL5x1aq5nQc12eV74ntDK7NzNMXR2cZ8Yd8DFVyJhpZwVavZkH7Ld1eycUygtfke57/Jl28fPBfLXKD+uG6vReBVWuFpNpHoMcnSPWocjvr10XLzGRim0f4De6IKQALTQBSEBNA+B9Yp6u01OTAqXaPDbqCLDpjLuyTXPMzJ/0VPBoassNogijzERfTbZChJnnGM4KLniWmD+jqhgwqBnMwGxOCHWnV/2QZvdYKe3rLrKhbOuTn2viBReyK3pPplOg1lwLGDYAirhma1588ICfsgpikOS0OapU8fb58+fpzNFzNgNvFB5rKjhGncWmtKoBMPnZUFMdq/FwmVbUKBE7jVBEFZCC10QEoAWuiAkgA04erjPU9wIbK9sR86Lp2WOnnPRxSz8zYB5OLrbypl1fc3nLj50bTIgFmv0ZETRV2L/nh6C0uwC26D+QtTazBi6yBY4D5x8gZ1BPxiqOq3C3ywK+v0wNdaEdy5Vd2bxQhqBsfNtFVzwENo5TcKlS5eOt3u9eDgxleegS+SNeTju63FiHjv0cyEPw7v5mpGUVm981TyY5twEeqMLQgLQQheEBHDyyLioeb65aXEEcuWYv0MULYUFEpamqTu2zZFKZGphMNX0wJruJuqIXYrsYQT3SMVZXXDNgrLnMnL/lWCWsYGHoomsDLOzZWvWo2XKTie8LXZftSnKcDKbBl1AOdSmO7qVMlhLPWY2e464PFwr7+JF51Kb4dgWh0Hd92UbaAlTlArum0caVX9Z485a5347Pg1f04t+g2w/ekbsmlJknCAIK6CFLggJQAtdEBJAY47OroAoMIx0g8HkJNxvjmUlmILrpedhJRhyH2E2VE4jzCMhsNxGVxzzeaxlzmGRBYWRoouIwySnUxc+e/a0y05bVcwAx4cZcUf3FXY7eZllqN5K2WE8ly3g03MaO3JQv9gmj8HNyYBqnqOK75iKavD3hqLAMNJwOGqLvtvEQmDZxegXXgjDKOrSPXsqMjC30e9cm4vA6o0uCClAC10QEoAWuiAkgBP70ZkDhneMJ63SztSK+D3J9zscO/9qq2P3ZhVW9LOzsmoPuLXxYS8LDoaL6yEnPxoE+ETp7+mMlGqQP3No6KDv4gV2IFR2eR/Enw1Hp4k23wnoPviXbEExxympz7RpLmfwnYL5suGkXkUeCjmFY/m+9g/24Sg72i6FQqPvvCCff46p1RRmW5DPHZ84j777krHuvBwSbIo1hsNj/TOG+TwW92wKvdEFIQFooQtCAmievRYphriZE42BIafhvpINTFJjnIM5k0OxgqP/CGcxcWgmhsByWCu77Yybhcy5Amqi12Sqs7HcgRBZU+M8y7ItyEhDUcuj8dRBN908ouTDLjLOQKvQLUZ9My4iAe41NilRfSb3wpvD4Z9FhJZxcQc28zGct02mewFuTRboxN9gOT4cOz0jbJ3j2uCwWzzPuvro9jml3w+evRNoQ+qNLggpQAtdEBKAFrogJIA/TppqhDT4ijIRguH5ecLn6fZtgYJef7AyDHKtegjzcGhjCuZqNRMIFe1QqCiowTDX71AaJvK6iq7ZB3cW81HGDDgyh91ieKpXyJGSRjGcuEXXnJHLsQZXHBfZMOG8nDpMPyi6twZbNgS2TyGxiCn9JrPS8fk8synIse8bzOc7EHbLYcAcPovtDs4Hp9wy14+0WX0Xv5vIvSYIwkpooQtCAtBCF4QEcPJKLbF9NzkOuksmbhHffd6xUlKtnvOr1/U4WgSvKMI+UuThXFSRv0WgD7lNPu4JqMuyj5b92DgGGNofzhvm5fzNYA7nnRMHRf7u+dE5PgDuBdN2l+Pp2viFKYTA8hzYuWWfcRYMrT13zkpkDbbcbz2bztakl4Z90TXc55x+2zlVqxlNJ+GUX7okhj9jSi1X9+lwH/nccbQxxdiZVGAFQVgFLXRBSADtl8OFFoN/GlSjISXVSPZaRgozGZhI9ZRNLQ6TBNPUU1epwmGtbGohJeCa7LFr8CRgAQCu4ADHcqEMDkedmWJ/FB6LIbD2CsbkX6CAOSCRFu/NgAUrmUq0MMyUXVI0J9vbO8fbr3/d60xfv+/M+snEmtiHQ1sMYwjtKSnGTsE8Z1dllUdcsh69s2b/CM47nJCqMMzlOvVd7O+wSw/cf/vDsNswBL3RBSEBaKELQgLQQheEBHDiENho0qpJqVvD4KvVlTQWMCyFuCKrh6AqyjolDww5ZV6J4/XOw6GikfvE6izMydkNNYew0sLLgXTnndNYp+QiQl7O7jUszMfhsV7RQJN+G5ccxbnl3wR5rleVhE67u+sUbvtUSBHPu7Vlr3FqZ4fGE1byGY4cfz84PDR9h9Qejyer1X6X1+Dnq2qmGkM3PQUX3rINxS35uUQP383bd7NNoTe6ICQALXRBSADN3WvcfomKF6HzhgseZtFot1bLme5eOWsyTTHazC/SsEkmHprVXMABi/2t+3sarrOO16jZBURnwTGUdB5jCHqKMuHCC17ddxYtxAw+dqGBicsFCXhOLl445/q8LD0seEg9kYePMwp3O87M3z1tTX5OCJuCGw9N/gXu3Llj2kOodz8cDk0fRuDxs8bUsAvjLQs7oCGM58q1a9mm0BtdEBKAFrogJAAtdEFIACd2r1l+wTwJeOUa9xoqZ3iow1fAQolH/wHuLHbl8LEw9pzGh+odrEzDxfYwJJXvcg5KMeuy11oQoluRCw/9UF5RvogrpyRVm7KCDDkOr+QJwoKMNPElZ9eZ54BdaEWwmGUPClMscPbs2aArzj5pzdWD+OPRzHzz8T762PFBNl0PMiMXOAOuwOV5gYdPKAR2BJx9f98Voli2Dy2fH0IBSf72MBoeHG/fui6OLgjCCmihC0IC0EIXhATw8ijMxHi513Wy4nFcQSRvd4MqpznxncMD5EacPtkK+q1zytnE0EzmjkgBOTyWObr3vSFwIj4Pz5fp9+YZ0l2Jn3LYrfmpKW3WDwtGXzn9JhBqzCGvp3d3TXvn9Klg3INpRb5LeIhU6Kk4foJUW2b1LOL/DqvAbg2sOvHOliuMef7CBdPHKjfD0XhlYckFzp09466xZa/RBHqjC0IC0EIXhATQ2HSPh7m+lCKLzf7q1JHa6ctjO84FMiMT1wthvH3bnZeyhLBW+ZwKHs7IZdUChZCYigyb2LOpPW+3C+ehDDBUf/E8QlF6FS5iyEo1DJNdx2Yq/0poykdqg2OBwwXuv++yaffBhcUCmUhJojRneU0cK52nhYpAbI7z0OE5oN/Po5j4WHiZkjg/8RrxW1C4YmvbmfwLXLh48Xj7dX/5+mxT6I0uCAlAC10QEoAWuiAkgJdQZLEZL+dicTHkUfmZ+N6tjuM3XArx9q1bpn2wt3e8PT20/H06cumAUyoWULIqbAZcm8NjUXkFQ0qPdjatCXB2FvlHVyEXEuDzYGQr79uCCfSGE4HHiSMuK7/4oNvu9+193X/fJbpOHqT6NADbjLnbYsU1c/qm0gqfp2BFmUhhiBgLr+dU+JLUZMvcPbmjie174hdPHm+/+R/+VbYp9EYXhASghS4ICeDk4pDrRB9D8Fw7rYg/JIv02QysouMi42bkFrtz15nqXPd8AhlDrCyCNc65Jtk6YIbYurnCmuNenThoezSIfUJwHS6QUJmsLntf0frfNB4eXwzoosLstFXuo5i5zvdCF4kIUoZR0D1zjTk0+zmSMGceCZmBsZrndcV9HJFYB6I3s+z53//+ePuv3mALXDSB3uiCkAC00AUhAWihC0ICyOuY4rwgCPcE9EYXhASghS4ICUALXRASgBa6ICQALXRBSABa6IKQALTQBSEBaKELQgLQQheE7N7H/wceJBRYrNi+kgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"Dataset len:\")\n",
    "print(len(test_dataset))\n",
    "\n",
    "print(\"\\nDataset classes:\")\n",
    "print(test_dataset.classes)\n",
    "\n",
    "print(\"\\nRandom image from dataset:\")\n",
    "img, label = test_dataset[random.randint(0, len(test_dataset)-1)]\n",
    "image = img.permute(1, 2, 0).numpy()  # [C, H, W] -> [H, W, C]\n",
    "\n",
    "# Denormalizing (there was normalization in transform)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "image = image * std + mean\n",
    "image = image.clip(0, 1)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLITING DATA INTO TRAINING AND VALIDATION DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75688, 32437, True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len = round(0.7 * len(train_dataset))\n",
    "val_len = len(train_dataset) - train_len\n",
    "train_len, val_len, train_len + val_len == len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_, val_dataset = random_split(train_dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 75688\n",
      "Number of validation samples: 32437\n",
      "Number of train classes: 26\n",
      "Number of val classes: 26\n",
      "Number of test samples: 674\n"
     ]
    }
   ],
   "source": [
    "# Sanity check once more\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset_)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of train classes: {len(train_dataset_.dataset.classes)}\")\n",
    "print(f\"Number of val classes: {len(val_dataset.dataset.classes)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "Validation dataset classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset classes: {train_dataset.classes}\")\n",
    "print(f\"Validation dataset classes: {train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested num_workers: 24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_workers = os.cpu_count()\n",
    "print(f\"Suggested num_workers: {num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset_, \n",
    "                              batch_size=32, \n",
    "                              shuffle=True, \n",
    "                              num_workers=0)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=32,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=32, \n",
    "                             shuffle=False, \n",
    "                             num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "def loader_sanity_check(loader):\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        print(f\"Batch nr: {batch_idx}\")\n",
    "        print(f\"Batch size: {data.shape[0]}\")\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "        print(f\"Image shape: {data[0].shape}\")\n",
    "        print(f\"Classes: {labels}\")\n",
    "        print(f\"Num classes: {len(labels)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nr: 0\n",
      "Batch size: 32\n",
      "Data shape: torch.Size([32, 3, 64, 64])\n",
      "Image shape: torch.Size([3, 64, 64])\n",
      "Classes: tensor([18, 11,  9,  9, 11, 20,  8, 20, 15,  2,  9,  1, 15, 23, 12,  1,  5, 15,\n",
      "        10, 24, 14,  2, 24,  5, 11, 11, 25,  5, 15, 13,  7, 12])\n",
      "Num classes: 32\n"
     ]
    }
   ],
   "source": [
    "loader_sanity_check(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nr: 0\n",
      "Batch size: 32\n",
      "Data shape: torch.Size([32, 3, 64, 64])\n",
      "Image shape: torch.Size([3, 64, 64])\n",
      "Classes: tensor([15,  9, 21,  6, 18, 15, 20,  8, 22,  0,  1, 10, 10,  7, 18,  1, 17,  7,\n",
      "        13, 17, 15, 22,  7,  0,  8,  0, 16, 21, 11, 19,  5, 19])\n",
      "Num classes: 32\n"
     ]
    }
   ],
   "source": [
    "loader_sanity_check(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch nr: 0\n",
      "Batch size: 32\n",
      "Data shape: torch.Size([32, 3, 64, 64])\n",
      "Image shape: torch.Size([3, 64, 64])\n",
      "Classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Num classes: 32\n"
     ]
    }
   ],
   "source": [
    "loader_sanity_check(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMET_ML SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : New validated dataset, Mobilenet test1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/ziemmi13/dlf-sign-letters-classification/fc6a9c0a60e7458faf682ebaef43da1d\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss : 9.718859672546387\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : New validated dataset, Mobilenet test1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size    : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate : 0.0001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_epochs    : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (4.58 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ziemmi13/dlf-sign-letters-classification/a1e98f8c21344b4c8541207dc7073dfa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "# Initialize Comet.ml experiment\n",
    "experiment = Experiment(\n",
    "    api_key=os.getenv(\"COMET_API_KEY\"),\n",
    "    project_name=\"DLF-sign_letters_classification\",\n",
    ")\n",
    "\n",
    "experiment.set_name(\"New validated dataset, Mobilenet test1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions to logs gradients and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.detach().numpy()\n",
    "\n",
    "\n",
    "def update_gradient_map(model, gradmap):\n",
    "    for name, layer in zip(model._modules, model.children()):\n",
    "        if \"activ\" in name:\n",
    "            continue\n",
    "\n",
    "        if not hasattr(layer, \"weight\"):\n",
    "            continue\n",
    "\n",
    "        wname = \"%s/%s.%s\" % (\"gradient\", name, \"weight\")\n",
    "        bname = \"%s/%s.%s\" % (\"gradient\", name, \"bias\")\n",
    "\n",
    "        gradmap.setdefault(wname, 0)\n",
    "        gradmap.setdefault(bname, 0)\n",
    "\n",
    "        gradmap[wname] += layer.weight.grad\n",
    "        gradmap[bname] += layer.bias.grad\n",
    "\n",
    "    return gradmap\n",
    "\n",
    "\n",
    "def log_gradients(gradmap, step):\n",
    "    for k, v in gradmap.items():\n",
    "        experiment.log_histogram_3d(to_numpy(v), name=k, step=step)\n",
    "\n",
    "\n",
    "def log_weights(model, step):\n",
    "    for name, layer in zip(model._modules, model.children()):\n",
    "        if \"activ\" in name:\n",
    "            continue\n",
    "\n",
    "        if not hasattr(layer, \"weight\"):\n",
    "            continue\n",
    "\n",
    "        wname = \"%s.%s\" % (name, \"weight\")\n",
    "        bname = \"%s.%s\" % (name, \"bias\")\n",
    "\n",
    "        experiment.log_histogram_3d(to_numpy(layer.weight), name=wname, step=step)\n",
    "        experiment.log_histogram_3d(to_numpy(layer.bias), name=bname, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRETRAINED MODEL (MOBILE_NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pretrained model (transfer learning)\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=weights).to(device)\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Ensure classifier requires grad\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Change classifier (fc) to satisfy aplication\n",
    "num_classes = len(train_dataset.classes)\n",
    "num_inputs = model.last_channel\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_inputs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(256, num_classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=26, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters: 3839490\n",
      "Frozen parameters: 2223872\n",
      "Trainable parameters: 1615618\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "print(f\"All parameters: {total_params}\")\n",
    "print(f\"Frozen parameters: {frozen_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4) # https://paperswithcode.com/method/weight-decay\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3) # https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "num_epochs = 3\n",
    "\n",
    "hyper_params = {\"batch_size\": batch_size, \"num_epochs\": num_epochs, \"learning_rate\": learning_rate}\n",
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, resume_path=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Load checkpoint if provided\n",
    "    start_epoch = 0\n",
    "    if resume_path:\n",
    "        checkpoint = torch.load(resume_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Starting Training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate batch loss (not divided by dataset size)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "           # Log every 50 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "                experiment.log_metric(\"train_loss\", loss.item(), step=batch_idx + len(train_loader) * batch_idx)\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = running_loss / len(train_loader)  # Average loss per batch\n",
    "        train_accuracy = 100. * train_correct / train_total\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Log epoch metrics\n",
    "        experiment.log_metric(\"epoch_train_loss\", epoch_loss, step=epoch)\n",
    "        experiment.log_metric(\"epoch_train_accuracy\", train_accuracy, step=epoch)\n",
    "\n",
    "        val_losses = []\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        print(\"Validating...\")\n",
    "        with torch.inference_mode():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        val_losses.append(val_epoch_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Training Loss: {epoch_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "        print(f'Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        print('-' * 60)\n",
    "\n",
    "        # Log validation metrics\n",
    "        experiment.log_metric(\"val_loss\", val_epoch_loss, step=epoch)\n",
    "        experiment.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = f\"./models/save_epoch_{epoch}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        model_path = f\"./models/New vaidated dataset MobileNet model, epoch {epoch}\"\n",
    "        weights_path = f\"./models/New vaidated dataset MobileNet weights, epoch {epoch}\"\n",
    "\n",
    "        torch.save(model, model_path)\n",
    "        torch.save(model.state_dict(), weights_path)\n",
    "\n",
    "        print(f\"Model checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "def test_model(device, model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    # Log test metrics\n",
    "    experiment.log_metric(\"test_loss\", test_loss)\n",
    "    experiment.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Epoch: 1, Batch: 0, Loss: 9.2058\n",
      "Epoch: 1, Batch: 100, Loss: 8.9996\n",
      "Epoch: 1, Batch: 200, Loss: 7.3834\n",
      "Epoch: 1, Batch: 300, Loss: 6.5608\n",
      "Epoch: 1, Batch: 400, Loss: 6.0558\n",
      "Epoch: 1, Batch: 500, Loss: 5.4208\n",
      "Epoch: 1, Batch: 600, Loss: 4.8655\n",
      "Epoch: 1, Batch: 700, Loss: 4.5407\n",
      "Epoch: 1, Batch: 800, Loss: 4.3952\n",
      "Epoch: 1, Batch: 900, Loss: 3.8147\n",
      "Epoch: 1, Batch: 1000, Loss: 3.6217\n",
      "Epoch: 1, Batch: 1100, Loss: 3.3520\n",
      "Epoch: 1, Batch: 1200, Loss: 3.4900\n",
      "Epoch: 1, Batch: 1300, Loss: 2.9089\n",
      "Epoch: 1, Batch: 1400, Loss: 2.7392\n",
      "Epoch: 1, Batch: 1500, Loss: 2.7593\n",
      "Epoch: 1, Batch: 1600, Loss: 2.1705\n",
      "Epoch: 1, Batch: 1700, Loss: 2.6217\n",
      "Epoch: 1, Batch: 1800, Loss: 2.5039\n",
      "Epoch: 1, Batch: 1900, Loss: 2.1294\n",
      "Epoch: 1, Batch: 2000, Loss: 2.2521\n",
      "Epoch: 1, Batch: 2100, Loss: 2.2361\n",
      "Epoch: 1, Batch: 2200, Loss: 2.3967\n",
      "Epoch: 1, Batch: 2300, Loss: 1.8356\n",
      "Validating...\n",
      "Epoch [1/3]\n",
      "Training Loss: 3.9967, Training Accuracy: 20.56%\n",
      "Validation Loss: 1.9715, Validation Accuracy: 51.94%\n",
      "------------------------------------------------------------\n",
      "Model checkpoint saved at: ./models/save_epoch_0.pt\n",
      "Epoch: 2, Batch: 0, Loss: 2.0709\n",
      "Epoch: 2, Batch: 100, Loss: 1.8454\n",
      "Epoch: 2, Batch: 200, Loss: 2.1466\n",
      "Epoch: 2, Batch: 300, Loss: 2.1778\n",
      "Epoch: 2, Batch: 400, Loss: 2.0684\n",
      "Epoch: 2, Batch: 500, Loss: 1.6084\n",
      "Epoch: 2, Batch: 600, Loss: 1.9021\n",
      "Epoch: 2, Batch: 700, Loss: 1.9082\n",
      "Epoch: 2, Batch: 800, Loss: 1.4895\n",
      "Epoch: 2, Batch: 900, Loss: 1.6198\n",
      "Epoch: 2, Batch: 1000, Loss: 1.8554\n",
      "Epoch: 2, Batch: 1100, Loss: 2.0669\n",
      "Epoch: 2, Batch: 1200, Loss: 1.7568\n",
      "Epoch: 2, Batch: 1300, Loss: 1.8630\n",
      "Epoch: 2, Batch: 1400, Loss: 1.6482\n",
      "Epoch: 2, Batch: 1500, Loss: 1.6936\n",
      "Epoch: 2, Batch: 1600, Loss: 1.7010\n",
      "Epoch: 2, Batch: 1700, Loss: 1.5006\n",
      "Epoch: 2, Batch: 1800, Loss: 1.3674\n",
      "Epoch: 2, Batch: 1900, Loss: 1.3300\n",
      "Epoch: 2, Batch: 2000, Loss: 1.6315\n",
      "Epoch: 2, Batch: 2100, Loss: 1.4191\n",
      "Epoch: 2, Batch: 2200, Loss: 1.3563\n",
      "Epoch: 2, Batch: 2300, Loss: 1.3197\n",
      "Validating...\n",
      "Epoch [2/3]\n",
      "Training Loss: 1.6544, Training Accuracy: 58.43%\n",
      "Validation Loss: 1.2189, Validation Accuracy: 71.94%\n",
      "------------------------------------------------------------\n",
      "Model checkpoint saved at: ./models/save_epoch_1.pt\n",
      "Epoch: 3, Batch: 0, Loss: 1.1204\n",
      "Epoch: 3, Batch: 100, Loss: 1.3396\n",
      "Epoch: 3, Batch: 200, Loss: 1.6441\n",
      "Epoch: 3, Batch: 300, Loss: 1.5359\n",
      "Epoch: 3, Batch: 400, Loss: 1.1051\n",
      "Epoch: 3, Batch: 500, Loss: 1.4554\n",
      "Epoch: 3, Batch: 600, Loss: 1.0422\n",
      "Epoch: 3, Batch: 700, Loss: 1.3761\n",
      "Epoch: 3, Batch: 800, Loss: 1.0615\n",
      "Epoch: 3, Batch: 900, Loss: 1.1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Due to connectivity issues, there's an error in processing the heartbeat. The experiment's status updates might be inaccurate until the connection issues are resolved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 1000, Loss: 1.4043\n",
      "Epoch: 3, Batch: 1100, Loss: 1.0910\n",
      "Epoch: 3, Batch: 1200, Loss: 1.2253\n",
      "Epoch: 3, Batch: 1300, Loss: 1.4265\n",
      "Epoch: 3, Batch: 1400, Loss: 1.2155\n",
      "Epoch: 3, Batch: 1500, Loss: 0.8954\n",
      "Epoch: 3, Batch: 1600, Loss: 1.0271\n",
      "Epoch: 3, Batch: 1700, Loss: 1.2099\n",
      "Epoch: 3, Batch: 1800, Loss: 0.8983\n",
      "Epoch: 3, Batch: 1900, Loss: 1.0882\n",
      "Epoch: 3, Batch: 2000, Loss: 1.0083\n",
      "Epoch: 3, Batch: 2100, Loss: 1.2282\n",
      "Epoch: 3, Batch: 2200, Loss: 0.8663\n",
      "Epoch: 3, Batch: 2300, Loss: 1.0330\n",
      "Validating...\n",
      "Epoch [3/3]\n",
      "Training Loss: 1.2333, Training Accuracy: 69.58%\n",
      "Validation Loss: 0.9495, Validation Accuracy: 79.18%\n",
      "------------------------------------------------------------\n",
      "Model checkpoint saved at: ./models/save_epoch_2.pt\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model(device, model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model and weights\n",
    "torch.save(model, \"./models/New vaidated dataset MobileNet test1\")\n",
    "torch.save(model.state_dict(), \"./models/New validated dataset MobileNet test1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the Model to Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "log_model(experiment, model, \"New validated dataset MobileNet test1 - sign letters classification model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : New validated dataset, Mobilenet test1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/ziemmi13/dlf-sign-letters-classification/a1e98f8c21344b4c8541207dc7073dfa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch_train_accuracy [3] : (20.563365394778565, 69.5843462636085)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch_train_loss [3]     : (1.2333032162332576, 3.9967047474390354)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [72]          : (0.8663419485092163, 9.205835342407227)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_accuracy [3]         : (51.94376791935136, 79.17501618522058)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [3]             : (0.9495286483912778, 1.9714663522483329)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : New validated dataset, Mobilenet test1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size    : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate : 0.0001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_epochs    : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (4.58 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element            : 2 (14.88 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 file(s), remaining 14.01 MB/14.88 MB\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 9.36 MB/14.88 MB, Throughput 316.48 KB/s, ETA ~31s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 4.47 MB/14.88 MB, Throughput 333.51 KB/s, ETA ~14s\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    # transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Values calculated from the ImageNet dataset\n",
    "])\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(self.csv_file)\n",
    "        self.classes = sorted(np.unique(self.df[\"class_index\"]))\n",
    "        self.class_names = sorted(np.unique(self.df[\"class_name\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        dataset_len = sum([len(os.listdir(self.root_dir + class_)) for class_ in self.class_names])\n",
    "        return 670\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        try:\n",
    "            # Use iloc for position-based indexing\n",
    "            label = self.df.iloc[index][\"class_index\"]\n",
    "            image_path = self.df.iloc[index][\"image_path\"]\n",
    "\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(image_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "            # Load image\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to read image at: {image_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Apply transform\n",
    "            if self.transform:\n",
    "                image = self.transform(Image.fromarray(image))\n",
    "\n",
    "            return image, label\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data at index {index}: {e}\")\n",
    "            raise\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0780, Test Accuracy: 29.85%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = test_model(device, model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=26, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.load(f=\"./models/New vaidated dataset MobileNet model, epoch 2\", weights_only=False)\n",
    "loaded_model.to(device)\n",
    "print(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 18\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Funkcja do przewidywania klasy pojedynczego obrazu\n",
    "def predict_image(image_path, model, device, class_names):\n",
    "    # Wczytanie i przetworzenie obrazu\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Rozmiar wejścia dla ResNet50\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalizacja ImageNet\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')  # Konwersja na RGB\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Dodanie wymiaru wsadowego\n",
    "\n",
    "    model.to(device)\n",
    "    # Przewidywanie\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "    # Zwracanie nazwy klasy\n",
    "    predicted_class = class_names[predicted.item()]\n",
    "    return predicted_class\n",
    "\n",
    "# Przykład użycia\n",
    "image_path = \"./dataset/test_images/y.png\"\n",
    "class_names = train_dataset.classes  # Zakładając, że masz listę klas\n",
    "predicted_class = predict_image(image_path, loaded_model, device, class_names)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=26, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.load(f=\"./models/New vaidated dataset MobileNet model, epoch 2\", weights_only=False)\n",
    "loaded_model.to(device)\n",
    "print(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "def test_model(device, model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model, device, class_names):\n",
    "\n",
    "    # image = img.convert('RGB')  # Konwersja na RGB\n",
    "    image_tensor = img.unsqueeze(0).to(device)  # Dodanie wymiaru wsadowego\n",
    "\n",
    "    model.to(device)\n",
    "    # Przewidywanie\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "    # Zwracanie nazwy klasy\n",
    "    predicted_class = class_names[predicted.item()]\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    MediapipeHandCrop(max_num_hands=1, min_detection_confidence=0.5),  \n",
    "    transforms.Resize((64,64)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class NewTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.csv_file = csv_file\n",
    "        self.df = pd.read_csv(self.csv_file)\n",
    "        self.classes = sorted([class_name for class_name in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir, class_name))])\n",
    "\n",
    "        # Maping class names to numbers \n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.classes) if os.path.isdir(os.path.join(self.root_dir, class_name))}\n",
    "\n",
    "    def __len__(self):\n",
    "        # Zsumuj liczbę plików w podkatalogach\n",
    "        dataset_len = sum(\n",
    "            len(os.listdir(os.path.join(self.root_dir, class_)))\n",
    "            for class_ in self.classes\n",
    "            if os.path.isdir(os.path.join(self.root_dir, class_))\n",
    "        )\n",
    "        return dataset_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        \n",
    "        image_path = self.df.loc[idx, \"file_path\"]\n",
    "        \n",
    "        # Debuging label\n",
    "        # label = os.path.join(self.root_dir,\n",
    "        #                      self.df.loc[idx, 'label'])\n",
    "        label = self.class_to_idx[self.df.loc[idx, 'label']]\n",
    "                                                     \n",
    "         # Loading image with cv2\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        \n",
    "        # Converting to RGB\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # # Transform\n",
    "        if self.transform:\n",
    "            image = self.transform(Image.fromarray(image))\n",
    "        \n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"./asl_test_dataset\"\n",
    "CSV_FILE = \"./asl_test_dataset/asl_dataset_csv.csv\"\n",
    "\n",
    "# Train dataset and dataloader initialization\n",
    "new_test_dataset = NewTestDataset(root_dir=ROOT_DIR, \n",
    "                        csv_file=CSV_FILE, \n",
    "                        transform=transform,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input image must contain three channel rgb data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_model_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_test_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[163], line 5\u001b[0m, in \u001b[0;36mvisualize_model_predictions\u001b[1;34m(model, test_dataset)\u001b[0m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Fetch image\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     img, label \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m     image \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# [C, H, W] -> [H, W, C]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     class_names \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mclasses\n",
      "Cell \u001b[1;32mIn[175], line 60\u001b[0m, in \u001b[0;36mNewTestDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Apply transformations if defined\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 60\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Hyperbook\\Desktop\\STUDIA\\DLF3\\DLF-sign_letters_classification\\mediapipe_handcrop.py:38\u001b[0m, in \u001b[0;36mMediapipeHandCrop.__call__\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput should be a PIL.Image, numpy.ndarray, or a valid file path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Przetwarzanie obrazu za pomocą MediaPipe\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmp_hands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hand_landmarks \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;66;03m# Get the bounding box around the hand\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solution_base.py:329\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (input_stream_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE_FRAME \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    327\u001b[0m       input_stream_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE):\n\u001b[0;32m    328\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m RGB_CHANNELS:\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel rgb data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    330\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    331\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    332\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    333\u001b[0m                                data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input image must contain three channel rgb data."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_model_predictions(model=loaded_model,\n",
    "                            test_dataset=new_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(new_test_dataset, \n",
    "                             batch_size=32, \n",
    "                             shuffle=False, \n",
    "                             num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1393, Test Accuracy: 10.80%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_accuracy = test_model(device, loaded_model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
